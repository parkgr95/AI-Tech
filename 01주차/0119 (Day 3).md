# 1. 1월19일 (Day 3) 학습 정리

### AI Math 5강 딥러닝 학습방법 이해하기

- (복습) 선형모델

  ![image-20220119141643612](https://user-images.githubusercontent.com/62732145/150140513-47f23f54-cb30-4cc4-a9a1-6179133c9f24.png)

  - O : 출력 변수.

  - X : 입력 변수. 데이터 항 

  - W : 가중치 행렬. 차원을 d에서 p로 바꿔주는 p차원 파라미터 벡터.

  - b : 오차항, 노이즈. XW의 모든 원소에 값을 더해준다. 

- **소프트맥스(softmax) 함수** 란? 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산을 말한다. 데이터가 어떤 클래스에 속하는 지 분류해준다.

    ![image-20220119142557954](https://user-images.githubusercontent.com/62732145/150140645-7f57853d-ffb1-422a-b1c3-05469b5ceeec.png)
    ![image-20220119143932554](https://user-images.githubusercontent.com/62732145/150140659-3f2ef3cf-1cb5-41b2-90b9-10c0ff11497a.png)
  - 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측한다.

  - 소프트맥스 함수를 통해 p차원에 있는 벡터를 확률벡터로 바꿀 수 있다. 확률 벡터가 되기에 어떤 클래스 k에 속할 확률로 해석할 수 있다.

  - but! 추론을 할 때는 **원-핫(one-hot) 벡터** 를 사용하면 된다. 학습을 할 때는 소프트맥스 사용! 

- 비선형모델

- **신경망(Neural Network)** 이란? 선형모델과 **활성함수(activation function)** 를 합성한 함수이다.

    ![image-20220119144620820](https://user-images.githubusercontent.com/62732145/150140722-b4dc2050-4215-469f-aee0-2f276df565f8.png)

    ![image-20220119144354453](https://user-images.githubusercontent.com/62732145/150141600-72da95e0-9fa6-442d-b673-e22f7a3bcae8.png)

  - 선형 모델과 활성함수를 합성한 함수를 잠재 벡터, 히든 벡터, 뉴런이라고 한다.

  - 잠재벡터 H를 다시 한 번 선형변환해 출력하게 되면 (W(2), W(1))를 패러미터로 가진 2층(2-layers) 신경망이다. 이를 퍼셉트론이라 한다.

  - 신경망이 여러층이 합성하면, 현재 딥러닝의 기본 모델인 **다층(multi-layer) 퍼셉트론(MLP)** 이라고 한다.

  - MLP는 L개의 가중치 행렬과, b의 절편 파라미터로 구성되어 있다.

- **활성함수(activation function)** 란? 선형모형과 차이를 두기 위해 사용하는 비선형함수이다.

  - 딥러닝에서는 ReLU 함수를 많이 사용하고 있다.

  - 하나의 실수값만을 input으로 받는다.

  - 소프트맥스 함수와의 차이? 
    - 소프트맥스 - 출력물의 모든 값을 고려해서 벡터를 받아서 출력한다
    - 활성함수 - 하나의 주소만 고려해 실수값 1개를 받아서 출력한다.

  - 실수만 받기 때문에, z 행렬의 모든 원소의 활성함수를 씌운다.
  - 층을 여러개를 쌓는 이유? 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능합니다.

  - 단, 최적화가 쉽다는 말은 아니다.

  - 이렇게 층을 쌓으며 계산해가는 과정을 **순전파(propagation)** 이라고 한다.

- **역전파(backpropagation) 알고리즘**

  ![image-20220119152208040](https://user-images.githubusercontent.com/62732145/150140763-51d22dd3-cee5-40be-9d24-9f9beede5d97.png)

  - 딥러닝은 역전파 알고리즘을 이용해 각 층에 사용된 패러미터들을 학습한다.

  - 각 층 패러미터의 그레디언트 벡터를 윗층부터 역순으로 계산하는데, 역전파는 **연쇄법칙(chain-rule)** 을 통해 그레디언트 벡터를 전달한다.

  - 각 뉴런의 텐서 값을 컴퓨터가 기억해야 미분 계산이 가능하기에, 역전파 알고리즘은 많은 메모리를 사용한다.

  - 역전파를 수행하며 SGD, 미니배치를 사용해 목적식을 최소화한다.  

- **연쇄법칙(chain-rule)** 이란? 합성함수의 미분법으로, 역전파 알고리즘의 자동미분(auto-differentiation)에 사용된다.

### AI Math 6강 확률론 맛보기

- 딥러닝에서 확률론이 필요한 이유?

  - 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.

  - 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터공간을 통계적으로 해석해서 유도한다.

  - 회귀 분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도한다.

  - 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다.

  - 즉, 분산 및 불확실성을 최소화하기 위해 측정하는 방법을 알아야 한다.

- 이산확률변수 vs 연속확률변수

  - 확률변수는 확률분포 𝒟에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분한다.

  - 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링 한다.

    ![image-20220119154941016](https://user-images.githubusercontent.com/62732145/150140971-b9a83c99-bc54-43f7-9306-4912d1136f45.png)

  - P(X = x) : 확률 질량 함수

  - 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다.

    ![image-20220119155048352](https://user-images.githubusercontent.com/62732145/150140990-f72f0a86-a7b6-4a14-9f28-a84759e96bc6.png)

  - P(x) : 밀도 함수, 누적확률분포의 변화율

  - 모델링 방법에 따라 이산확률변수나 연속확률변수로 사용할 수 있다.

- 확률분포는 데이터의 초상화

  - 데이터공간을 𝒳×𝒴라 표기하고 𝒟는 데이터공간에서 데이터를 추출하는 분포이다.

  - 데이터는 확률변수로 (x,y) ∼ 𝒟라 표기한다.

  - 결합분포 P(x,y)는 𝒟를 모델링한다. 
    - 결합분포는 확률분포의 종류(이산, 연속)에 상관없이 모델링을 따른다.

  - P(x)는 입력 x에 대한 주변확률분포로 y에 대한 정보를 주지 않는다.

    - 주변확률분포는 결합분포를 적분해서 유도할 수 있다.

      ![image-20220119221602229](https://user-images.githubusercontent.com/62732145/150141008-202036ec-5951-44c8-9934-8311cec18fe5.png)

  - 조건부확률분포 P(x|y)는 데이터 공간에서 입력 x와 출력 y 사이의 관계를 모델링한다.
    - 즉, 입력과 출력 사이의 통계적 관계를 모델링 할 때 사용된다.

- **조건부확률**

  - 조건부확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률을 의미한다.
    - 연속확률분포일 경우 확률이 아닌 밀도로 해석한다!

  - 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용된다.

  - 분류 문제에서 softmax(Wφ + b)은 데이터 x로부터 추출된 특징패턴 φ(x)과 가중치행렬 W을 통해 조건부확률 P(y|x)을 계산할 수 있다.

  - 회귀 문제의 경우 조건부기대값 𝔼[y|x]을 추정한다.

  - 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 φ을 추출한다.

- **기대값(expectation)**이란?

  - 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 계산할 수 있다.

  - 그 중 기대값은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.

  - 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있습니다.

- **몬테카를로 샘플링**

  - 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이다.

  - 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용해야 한다.

    ![image-20220119222613909](https://user-images.githubusercontent.com/62732145/150141526-2df41ab2-ea02-4a67-8714-e2315d6bf178.png)

  - 몬테카를로 샘플링은 독립추출만 보장된다면 **대수의 법칙(law of large number)** 에 의해 수렴성을 보장한다.
    - 대수의 법칙(라플라스의 정리) : 큰 모집단에서 무작위로 뽑은 표본의 평균이 전체 모집단의 평균과 가까울 가능성이 높다.

# 2. 피어세션 정리

### 강의 리뷰 및 Q&A

- AI Math 5강 딥러닝 학습방법 이해하기
- AI Math 6강 확률론 맛보기
- Python 3-1강 Python Data Structure

### 그리고

- AI Math 공부 방향성 정하기
- 멘토님께 질문할 내용 정리
- 1월 21일 (금) 경사하강법, 확률론 리뷰

# 3. 퀴즈 결과 회고

### AI Math 5강 퀴즈 - 딥러닝 학습방법 이해하기

1. ReLU 계산
2. tahn'(x) 계산
3. 역전파 알고리즘의 기반은? 연쇄 법칙
4. 신경망에서 활성함수가 필요한 이유는? 비선형 근사를 하기 위해서
5. 연쇄 법칙 미분 계산하기

### AI Math 6강 퀴즈 - 확률론 맛보기

1. 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다.
2. 연속형 확률변수의 한 지점에서의 밀도는 그 자체로 확률값을 가지지 않는다.
3. 몬테카를로 샘플링 방법은 변수 유형에 상관없이 사용할 수 있다.
4. 기대값 계산
5. 분산 계산

# 4. 총평

확률론의 공부가 완료되지 않았습니다. 복습이 필요합니다.
