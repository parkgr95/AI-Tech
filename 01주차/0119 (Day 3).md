# 1. 1월 19일 (Day 3) 강의 정리

### (AI Math 5강) 딥러닝 학습방법 이해하기

- **소프트맥스(softmax) 함수**란?

  - 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산이다. 

    ![image-20220119142557954](https://user-images.githubusercontent.com/62732145/151789342-bb7ebd50-daf2-4a53-8ef1-32aa037c22c4.png)

    - 출력벡터 *O* 에 softmax 함수를 합성하면 확률벡터가 되므로 특정클래스 *k*에 속할 확률로 해석할 수 있다.

    ![image-20220119143932554](https://user-images.githubusercontent.com/62732145/151789370-a4cced2a-0a50-47b7-bed0-58f63a8956f4.png)

  - 데이터가 어떤 클래스에 속하는 지 분류할 때 선형모텔과 소프맥스 함수를 결합하여  예측한다.

  - but! 추론을 할 때는 최대값을 가진 주소만 1로 출력하는**원-핫(one-hot) 벡터**로 연산하면 된다. 학습을 할 때는 소프트맥스 사용!

- **신경망(Neural Network)**이란?

  - 선형모델과 **활성함수(activation function)**를 합성한 함수이다.

    ![image-20220119144620820](https://user-images.githubusercontent.com/62732145/151789457-5a9efacc-5064-4243-a6d9-f240f4ff3cde.png)

  - **활성함수(activation function)**란?

    -  선형모형과 차이를 두기 위해 사용하는 비선형함수이다.

    -  시그모이드(sigmoid) 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에서는 ReLU 함수를 많이 사용하고 있다.

    -  하나의 실수값만을 input으로 받는다.

    -  소프트맥스 함수와의 차이? 
       - 소프트맥스 - 출력물의 모든 값을 고려해서 벡터를 받아서 출력한다
       - 활성함수 - 하나의 주소만 고려해 실수값 1개를 받아서 출력한다.

  - 선형 모델과 활성함수를 합성한 *H*는 잠재(히든) 벡터, 뉴런이라고 한다.

    ![image-20220119144354453](https://user-images.githubusercontent.com/62732145/151789526-582e5630-e08b-4189-9fb6-d59c1f033d28.png)

  - 잠재벡터 *H*를 다시 한 번 선형변환해 출력하게 되면 *(W(2), W(1))*를 패러미터로 가진 2층(2-layers) 신경망이 된다. 이 신경망을 퍼셉트론이라 한다.

    ![image-20220123011458201](https://user-images.githubusercontent.com/62732145/151789649-d361c0cd-0959-4100-8e6a-95eefa4d6413.png)

  - 신경망을 여러층 합성한 게 현재 딥러닝의 기본 모델인 **다층(multi-layer) 퍼셉트론(MLP)**이다.

- 층을 여러개를 쌓는 이유?

  -  층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능하다.
  -  단, 이 말이 최적화가 쉽다는 말은 아니다.
  -  이렇게 층을 쌓으며 계산해가는 과정을 **순전파(propagation)**이라고 한다.

- **역전파(backpropagation) 알고리즘**

  - 딥러닝은 역전파 알고리즘을 이용해 각 층에 사용된 패러미터들을 학습한다.

    ![image-20220119152208040](https://user-images.githubusercontent.com/62732145/151789700-3d1d3351-bd22-4698-a63d-5ee32f7eb914.png)

  - 역전파는 **연쇄법칙(chain-rule)** 기반 자동미분(auto-differentiation)을 사용해 각 층 패러미터의 그레디언트 벡터를 윗층부터 역순으로 계산하고, 전달한다.

  - 각 노드의 텐서 값을 컴퓨터가 기억해야 미분 계산이 가능하기에, 역전파 알고리즘은 많은 메모리를 사용한다.

  - 역전파를 수행하며 SGD, 미니배치를 사용해 목적식을 최소화한다.  

### (AI Math 6강) 확률론 맛보기

- 딥러닝에서 확률론이 필요한 이유?

  - 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.

  - 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터공간을 통계적으로 해석해서 유도한다.

  - 회귀 분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도한다.

  - 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다.

  - 즉, 분산 및 불확실성을 최소화하기 위해 측정하는 방법을 알아야 한다.

- 확률분포는 데이터의 초상화

  - 데이터공간을 𝒳 × 𝒴라 표기하고 𝒟는 데이터공간에서 데이터를 추출하는 분포이다.

  - 데이터는 확률변수로 (x,y) ∼ 𝒟라 표기한다.

    - (x, y) ∈ 𝒳 × 𝒴는 데이터 공간 상의 관측가능한 데이터

  - 이산확률변수 vs 연속확률변수

    - 확률변수는 확률분포 𝒟에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분한다.

    - 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링 한다.

      ![image-20220119154941016](https://user-images.githubusercontent.com/62732145/151789762-c5d9f3f4-e5c1-4978-a70a-cbf7c392c508.png)

      - P(X = x) : 확률변수가 x값을 가질 확률

    - 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링한다.

      ![image-20220119154941016](https://user-images.githubusercontent.com/62732145/151789782-c46f9ebe-a91a-4e54-bca0-86530fa3c908.png)

      - P(x) : 밀도 함수, 누적확률분포의 변화율

    - 모델링 방법에 따라 이산확률변수나 연속확률변수로 사용할 수 있다.

  - 결합분포 P(x,y)는 𝒟를 모델링한다. 

    - 결합분포는 확률분포의 종류(이산, 연속)에 상관없이 모델링을 따른다.

  - P(x)는 입력 x에 대한 주변확률분포로 y에 대한 정보를 주지 않는다.

    - 주변확률분포는 결합분포를 적분해서 유도할 수 있다.

      ![image-20220119221602229](https://user-images.githubusercontent.com/62732145/151789819-4eff0951-e611-497c-8758-f69c7505b7bd.png)

  - 조건부확률분포 P(x|y)는 데이터 공간에서 입력 x와 출력 y 사이의 관계를 모델링한다.

    - 즉, 입력과 출력 사이의 통계적 관계를 모델링 할 때 사용된다.

- **조건부확률**

  - 조건부확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률을 의미한다.
    - 연속확률분포일 경우 확률이 아닌 밀도로 해석한다!
  - 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용된다.
  - 분류 문제에서 softmax(Wφ + b)은 데이터 x로부터 추출된 특징패턴 φ(x)과 가중치행렬 W을 통해 조건부확률 P(y|x)을 계산할 수 있다.
  - 회귀 문제의 경우 조건부기대값 𝔼[y|x]을 추정한다.
  - **기대값(expectation)**이란?
    - 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 계산할 수 있다.
      - 범함수 : 함수들의 집합을 정의역으로 갖는 함수 (From Wiki)
    - 그 중 기대값은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.
    - 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있습니다.
  - 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 φ을 추출한다.

- **몬테카를로 샘플링**

  ​	![img](https://postfiles.pstatic.net/MjAyMTA4MTBfMjAz/MDAxNjI4NTUyMjE2ODg4.NY84whUURJTTUodKXgrI5XqSKCbaLNnJH_gKUCKs2V4g.Mw5NeCnGf-iVAaRxMYAX-JmWTzV3Eqk4-BYp-oXQfqUg.GIF.qhqoxogh/220px-Pi_30K.gif?type=w966)

  - 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이다.

  - 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용해야 한다.

    ![image-20220119222613909](https://user-images.githubusercontent.com/62732145/151789881-f88f0464-ce02-427a-8aa1-c4b57435d2c3.png)

    - 데이터공간에서 i.i.d(independent and identically distribute) 다시말해, 확률변수 (X1 , X2 , ... , Xn)가 상호독립적이며 모두 동일한 확률분포 P(x)를 가진다면 기대값은 랜덤하게 뽑은 N개 샘플의 평균치와 비슷하다.

      (출처) [몬테카를로 샘플링, 최대가능도 추정, 베이즈 통계학](https://blog.naver.com/qhqoxogh/222463975004)|**작성자** [깐깐징어](https://blog.naver.com/qhqoxogh)

  - 몬테카를로 샘플링은 독립추출만 보장된다면 **대수의 법칙(law of large number)**에 의해 수렴성을 보장한다.

    - 대수의 법칙(라플라스의 정리) : 큰 모집단에서 무작위로 뽑은 표본의 평균이 전체 모집단의 평균과 가까울 가능성이 높다.

### (Python 3-1강) Python Data Structure

- 스택 (Stack)

  - 나중에 넣은 데이터를 먼저 반환하도록 설계된 메모리 구조
  - Last In First Out (LIFO)
  - Data의 입력을 Push, 출력을 Pop이라고 함
  - 리스트를 사용하여 스택 구조를 구현 가능
  - push를 append(), pop을 pop()를 사용

- 큐 (Queue)

  - 먼저 넣은 데이터를 먼저 반환하도록 설계된 메모리 구조
  - First In First Out (FIFO)
  - Stack과 반대되는 개념
  - 파이썬은 리스트를 사용하여 큐 구조를 활용
  - put를 append(), get을 pop(0)를 사용

- 튜플 (tuple)

  - 값의 변경이 불가능한 리스트
  - 선언시“[]”가 아닌“()”를사용
  - 리스트의 연산, 인덱싱, 슬라이싱 등을 동일하게 사용
  - 왜 쓰는가?
    - 프로그램을 작동하는 동안 변경되지 않은 데이터의 저장
    - 함수의 반환 값등 사용자의 실수에 의한 에러를 사전에 방지

- 집합 (set)

  - 값을 순서없이 저장, 중복 불허 하는 자료형
  - set 객체 선언을 이용하여 객체 생성
  - 수학에서 활용하는 다양한 집합연산 가능

- 사전 (dictionary)

  - 데이터를 저장 할 때는 구분 지을 수 있는 값을 함께 저장
  - 구분을 위한 데이터 고유 값을 Identifier 또는 Key 라고함
  - Key 값을 활용하여, 데이터 값(Value)를 관리함
  - key와 value를 매칭하여 key로 value를 검색
  - 다른 언어에서는 Hash Table 이라는 용어를 사용
  - {Key1:Value1, Key2:Value2, Key3:Value3 ...} 형태

- collections

  - List, Tuple, Dict에 대한 Python Built-in 확장 자료 구조(모듈)
  - 편의성, 실행 효율 등을 사용자에게 제공함
  - 아래의 모듈이 존재함
    - from collections import deque
    - from collections import OrderedDict
    - from collections import defaultdict
    - from collections import Counter
    - from collections import namedtuple

- deque

  - Stack과 Queue를 지원하는 모듈
  - List에 비해 효율적인 = 빠른 자료 저장 방식을 지원함

  - rotate, reverse등 Linked List의 특성을 지원함
  - 기존 list 형태의 함수를 모두 지원함

  - deque는 기존 list보다 효율적인 자료구조를 제공

  - 효율적 메모리 구조로 처리 속도 향상

- OrderedDict

  - Dict와 달리, 데이터를 입력한 순서대로 dict를 반환함
  - 그러나 dict도 python 3.6부터 입력한 순서를 보장하여 출력함
  - Dict type의 값을, value 또는 key 값으로 정렬할 때 사용 가능

- defaultdict

  - Dict type의 값에 기본 값을 지정, 신규값 생성시 사용하는 방법

- Counter

  - Sequence type의 data element들의 갯수를 dict 형태로 반환
  - Dict type, keyword parameter 등도 모두 처리 가능
  - Set의 연산들을 지원함
  - word counter의 기능도 손쉽게 제공함

- namedtuple

  - Tuple 형태로 Data 구조체를 저장하는 방법
  - 저장되는 data의 variable을 사전에 지정해서 저장함

# 2. 퀴즈 리뷰

### [기본 퀴즈] AI Math 5강 퀴즈 - 딥러닝 학습방법 이해하기

1. ReLU 계산하기
2. tahn'(x) 계산하기
3. 역전파 알고리즘의 기반은? 연쇄 법칙
4. 신경망에서 활성함수가 필요한 이유는? 비선형 근사를 하기 위해서
5. 연쇄 법칙 미분 계산하기

### [기본 퀴즈] AI Math 6강 퀴즈 - 확률론 맛보기

1. 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다.
2. 연속형 확률변수의 한 지점에서의 밀도는 그 자체로 확률값을 가지지 않는다.
3. 몬테카를로 샘플링 방법은 변수 유형에 상관없이 사용할 수 있다.
4. 기대값 계산하기
5. 분산 계산하기

# 3. 피어세션 정리

- 강의 리뷰 및 Q&A

  - (AI Math 5강) 딥러닝 학습방법 이해하기

  - (AI Math 6강) 확률론 맛보기

  - (Python 3-1강) Python Data Structure

- AI Math 공부 방향성 정하기
- 멘토님께 질문할 내용 정리
- 1월 21일 (금) 경사하강법, 확률론 리뷰

# 4. 회고

확률론을 완벽히 이해하지 못했습니다. 복습이 필요합니다. 내일도 화이팅!
