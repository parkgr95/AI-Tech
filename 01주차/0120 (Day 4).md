# 1. 1월 21일 (Day 5) 강의 정리

### (AI Math 10강) RNN 첫걸음

- 시퀀스 데이터 이해하기

  - 소리, 문자열, 주가 등의 데이터를 시퀀스(sequence) 데이터로 분류합니다.
  - 시퀀스 데이터는 독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 됩니다.

- 시퀀스 데이터를 어떻게 다루나요?

  - 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있습니다.
  - 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요합니다.

- Recurrent Neural Network을 이해하기

  - 가장 기본적인 RNN 모형은 MLP와 유사한 모양입니다.
  - RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링합니다.
    - 가중치 행렬들은 t에 따라 변하지 않는다.
  - RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산합니다. (Backpropagation Through Time (BPTT))

- BPTT를 좀 더 살펴봅시다

  - BPTT를 통해 RNN의 가중치행렬의 미분을 계산해보면 아래와 같이 미분의 곱으로 이루어진 항이 계산됩니다

    # 1. 1월 21일 (Day 5) 강의 정리

### (AI Math 10강) RNN 첫걸음

- 시퀀스 데이터 이해하기

  - 소리, 문자열, 주가 등의 데이터를 시퀀스(sequence) 데이터로 분류합니다.
  - 시퀀스 데이터는 독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 됩니다.

- 시퀀스 데이터를 어떻게 다루나요?

  - 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있습니다.
  - 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요합니다.

- Recurrent Neural Network을 이해하기

  - 가장 기본적인 RNN 모형은 MLP와 유사한 모양입니다.
  - RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링합니다.
    - 가중치 행렬들은 t에 따라 변하지 않는다.
  - RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산합니다. (Backpropagation Through Time (BPTT))

- BPTT를 좀 더 살펴봅시다

  - BPTT를 통해 RNN의 가중치행렬의 미분을 계산해보면 아래와 같이 미분의 곱으로 이루어진 항이 계산됩니다

    ![image-20220131203648334](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220131203648334.png)

    - 불안정하기 쉬운 이유? 값들이 1보다 크면 무한정으로 커지고 그렇지 않으면 0이 되니까

- 기울기 소실의 해결책?

  - 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로 길이를 끊는 것이 필요합니다.
    - gradient가 0이 되는 즉, gradient vanishing이 일어나면 과거 시점의 정보를 잃어버리는 문제가 발생한다.
    - 이를 막기 위해 특정 과거의 정보를 몇 개의 블럭으로 나누는, truncated BPTT를 수행한다. 그렇지만 완결한 해결책은 아니다.
  - 이런 문제들 때문에 Vanilla RNN은 길이가 긴 시퀀스를 처리하는데 문제가 있습니다.
  - 이를 해결하기 위해 등장한 RNN 네트워크가 LSTM과 GRU 입니다.

# 2. 퀴즈 리뷰

### [기본 퀴즈] AI Math 10강 퀴즈 - RNN 첫걸음

1. 시퀀스 데이터 종류를 고르기
2. 시퀀스 데이터 **X**가 주어졌을 때, 베이즈 법칙에 따라 전개한 **X**에 대한 확률분포 고르기
3. RNN의 잠재변수는 다음 순서의 잠재변수를 모델링하기 위한 입력 값으로 사용될 수 있다.
4. 기울기 소실문제를 해결하기 위한 방안이 아닌 것 고르기
5. BPTT를 통해 RNN의 가중치행렬의 미분을 계산했을 때, 시퀀스 길이가 길어졌을 때 불안정해지기 쉬운 항을 고르기.

# 3. 과제 리뷰

### [심화 과제 1] Gradient Descent

### [심화 과제 2] Backpropagation

### [심화 과제 3] Maximum Likelihood Estimate

# 4. 스페셜 피어세션 정리

파이토치 코드 읽기 공유하기 

멘토링 :

부족한 수학 능력을 채우는 걸 먼저 추천.

리더보드는 에폭만 돌리기 때문에 점수가 높다고 잘하는 게 아니다. 그러니 이렇게 하면 어떻게 될 것이라는 근거에 따라 답을 찾아나가는 **감**을 찾아보자. 만들어보는 거 연습

피어세션때 팁: 친밀해져라. 정보공유가 안되서 어려울 수도 있다.

옆 사람과 비교 금지. 모르는 건 바로바로 물어보자

CV : 추천과 같이 쓴다.

따른 멘토 : 레벨 1때 시간많을 때, 논문 리딩 등, 파이썬 등도 확실히 하자

데이콘 같은 competition에 도전해 보는게 역량강화에 도움이 된다. 우선순위가 높다.

우리 멘토 : 면접 때 우리 회사의 기술 써봤는지 물어본다. 면접 땐 코테보단 cs지식

# 5. 피어세션 정리

- 스페셜 피어세션 회고

- 팀 회고 정리

- 심화과제를 어떤 식으로 할 것인가.
  -  해온 부분까지만 발표. 그 이상을 한 사람이 설명

# 6. 회고

주말을 통해 이해하지 못한 부분을 채우고 심화과제를 해결해야할 것 같습니다. 다음주도 화이팅!

    - 불안정하기 쉬운 이유? 값들이 1보다 크면 무한정으로 커지고 그렇지 않으면 0이 되니까

- 기울기 소실의 해결책?

  - 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로 길이를 끊는 것이 필요합니다.
    - gradient가 0이 되는 즉, gradient vanishing이 일어나면 과거 시점의 정보를 잃어버리는 문제가 발생한다.
    - 이를 막기 위해 특정 과거의 정보를 몇 개의 블럭으로 나누는, truncated BPTT를 수행한다. 그렇지만 완결한 해결책은 아니다.
  - 이런 문제들 때문에 Vanilla RNN은 길이가 긴 시퀀스를 처리하는데 문제가 있습니다.
  - 이를 해결하기 위해 등장한 RNN 네트워크가 LSTM과 GRU 입니다.

# 2. 퀴즈 리뷰

### [기본 퀴즈] AI Math 10강 퀴즈 - RNN 첫걸음

1. 시퀀스 데이터 종류를 고르기
2. 시퀀스 데이터 **X**가 주어졌을 때, 베이즈 법칙에 따라 전개한 **X**에 대한 확률분포 고르기
3. RNN의 잠재변수는 다음 순서의 잠재변수를 모델링하기 위한 입력 값으로 사용될 수 있다.
4. 기울기 소실문제를 해결하기 위한 방안이 아닌 것 고르기
5. BPTT를 통해 RNN의 가중치행렬의 미분을 계산했을 때, 시퀀스 길이가 길어졌을 때 불안정해지기 쉬운 항을 고르기.

# 3. 과제 리뷰

### [심화 과제 1] Gradient Descent

### [심화 과제 2] Backpropagation

### [심화 과제 3] Maximum Likelihood Estimate

# 4. 스페셜 피어세션 정리

파이토치 코드 읽기 공유하기 

멘토링 :

부족한 수학 능력을 채우는 걸 먼저 추천.

리더보드는 에폭만 돌리기 때문에 점수가 높다고 잘하는 게 아니다. 그러니 이렇게 하면 어떻게 될 것이라는 근거에 따라 답을 찾아나가는 **감**을 찾아보자. 만들어보는 거 연습

피어세션때 팁: 친밀해져라. 정보공유가 안되서 어려울 수도 있다.

옆 사람과 비교 금지. 모르는 건 바로바로 물어보자

CV : 추천과 같이 쓴다.

따른 멘토 : 레벨 1때 시간많을 때, 논문 리딩 등, 파이썬 등도 확실히 하자

데이콘 같은 competition에 도전해 보는게 역량강화에 도움이 된다. 우선순위가 높다.

우리 멘토 : 면접 때 우리 회사의 기술 써봤는지 물어본다. 면접 땐 코테보단 cs지식

# 5. 피어세션 정리

- 스페셜 피어세션 회고

- 팀 회고 정리

- 심화과제를 어떤 식으로 할 것인가.
  -  해온 부분까지만 발표. 그 이상을 한 사람이 설명

# 6. 회고

주말을 통해 이해하지 못한 부분을 채우고 심화과제를 해결해야할 것 같습니다. 다음주도 화이팅!
