# 1. 1월 20일 (Day 4) 강의 정리

### (AI Math 7강) 통계학 맛보기

- 모수란?

  - 통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표입니다.
  - 그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하므로, 근사적으로 확률분포를 추정할 수 밖에 없습니다.
    - 예측모형의 목적은 분포를 정확하게 맞추는 것보다는 데이터와 추정 방법의 불확실성을 고려해서 위험을 최소화하는 것이다.
  - 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적(parametric) 방 법론이라 합니다.
    - Ex) 정규분포의 모수 : 평균, 분산
  - 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수(nonparametric) 방법론이라 부릅니다.
    - 대부분의 기계학습 방법론
    - 주의! 비모수 방법론은 모수가 없는 게 아니다. 모수의 개수가 무한하거나, 유연하게 바뀌기 때문에 비모수 방법론이라고 하는 것!
  - 모수와 비모수 방법론의 차이는? 어떤 가정을 미리 부여 하는지 아닌지에 따라 구별된다.

- Ex) 확률분포 가정하기

  - 확률분포를 가정하는 방법: 우선 히스토그램을 통해 모양을 관찰합니다.
    - 데이터가 2개의 값(0 또는 1)만 가지는 경우 → 베르누이분포
    - 데이터가 n개의 이산적인 값을 가지는 경우 → 카테고리분포, 다항분포
    - 데이터가 [0,1] 사이에서 값을 가지는 경우 → 베타분포
    - 데이터가 0 이상의 값을 가지는 경우 → 감마분포, 로그정규분포 등
    - 데이터가 R 전체에서 값을 가지는 경우 → 정규분포, 라플라스분포 등
  - 기계적으로 확률분포를 가정해서는 안 되며, 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙입니다.
    - 모수를 추정한 후에는 각 분포마다 검정하는 방법들로 검정을 해야한다

- 데이터로 모수를 추정해보기

  - 데이터의 확률분포를 가정했다면 모수를 추정해볼 수 있습니다.

  - 정규분포의 모수는 평균 μ과 분산 σ^2으로 이를 추정하는 표본의 통계량(statistic) (추정량)은 다음과 같다.

    ​	![image-20220123163526660](https://user-images.githubusercontent.com/62732145/151790319-41ab1c9d-692a-49d8-92b6-a8393f2090e1.png)

    - 왜 표본분산은 N-1로 나눠준걸까? 그래야 모분산과 같아져 불편(unbiased) 추정량을 유도할 수 있기 때문이다. 
      - 표본평균이 정해지는 순간 x1,...,xn 중에서 n-1개가 정해지면 나머지 하나는 종속적으로 정해진다. 표본분산을 구할 때의 자유도는 n-1이다.
      - 불편 추정량 : 모수를 추정하는 값을 추정량의 기댓값이 모수와 같은 추정량

  - 통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균의 표집분포는 N 이 커질수록 정규분포 𝒩(μ, σ2/N)를 따릅니다.

    - 주의! 표집분포 != 표본분포
    - 모집단의 분포가 정규분포를 따르지 않아도 성립한다.

- 최대가능도 추정법

  - 표본평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 됩니다.
  - 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 최대가능도 추정법(maximum likelihood estimation, MLE)입니다.
    - 가능도 함수 : 모수 θ를 따르는 분포가 데이터 x를 관찰할 가능성. 확률로 해석하면 안된다! 
    - MLE는 불편추정량을 보장하진 않는다.
  - 데이터 집합 X 가 독립적으로 추출되었을 경우 로그가능도를 최적화합니다.

- 왜 로그가능도를 사용하나요?

  - 로그가능도를 최적화하는 모수 θ 는 가능도를 최적화하는 MLE 가 됩니다.
  - 데이터의 숫자가 적으면 상관없지만 만일 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능합니다.
  - 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해집니다.
  - 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그가능도를 사용하면 연산량을 O(n2) 에서 O(n) 으로 줄여줍니다.
  - 대게의 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 최적화하게 됩니다

- 딥러닝에서 최대가능도 추정법

  - 최대가능도 추정법을 이용해서 기계학습 모델을 학습할 수 있습니다.

  - 딥러닝 모델의 가중치를 θ = (W(1), ..., W(L))라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 (p1, ..., pK)를 모델링합니다.

  - 원핫벡터로 표현한 정답레이블 y = (y1, ..., yK) 을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있습니다.

    ![image-20220124003037624](https://user-images.githubusercontent.com/62732145/151790348-98b4cda9-481c-4a40-b43e-6ac3b4a4f460.png)

- 확률분포의 거리를 구해보자

  - 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도합니다.
  - 데이터공간에 두 개의 확률분포 P(x), Q(x) 가 있을 경우 두 확률분포 사이의 거리(distance)를 계산할 때 다음과 같은 함수들을 이용합니다.
    - 총변동 거리 (Total Variation Distance, TV)
    - 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)
      - 쿨백-라이블러 발산(KL Divergence)은 다음과 같이 정의합니다.
      - 쿨백 라이블러는 다음과 같이 분해할 수 있습니다.
      - 분류 문제에서 정답레이블을 P, 모델 예측을 Q 라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같습니다.
    - 바슈타인 거리 (Wasserstein Distance)

### (AI Math 8강) 베이즈 통계학 맛보기

- 조건부 확률이란?

  - 베이즈 통계학을 이해하기 위해선 조건부 확률의 개념을 이해해야 합니다.

  - 베이즈 정리는 조건부 확률을 이용하여 정보를 갱신하는 방법을 알려줍니다.

  - 조건부확률 P(A|B)는 사건 B가 일어난 상황에서 사건 A가 발생할 확률을 의미합니다.
    ![image-20220124003426718](https://user-images.githubusercontent.com/62732145/151790441-a9c2400b-c64d-44c4-8e91-00264cd4c2ab.png)

- 베이즈 정리를 통한 정보의 갱신
  - 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있습니다.
    <img width="1080" alt="스크린샷 2021-02-01 오전 9 58 20" src="https://user-images.githubusercontent.com/62732145/151790482-e1ec702a-7877-450b-a414-0bdae5ef99a4.png">
- 조건부 확률 -> 인과관계?
  - 조건부 확률은 유용한 통계적 해석을 제공하지만 인과관계(causality)를 추론할 때 함부로 사용해서는 안 됩니다.
    - 데이터가 많아져도 조건부 확률만 가지고 인과관계를 추론하는 것은 불가능합니다.
  - 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요합니다.
    - 단, 인과관계만으로는 높은 예측 정확도를 담보하기는 어렵습니다.
  - 인과관계를 알아내기 위해서는 중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 합니다.

### (AI Math 9강) CNN 첫걸음

- Convolution 연산 이해하기

  - 지금까지 배운 다층신경망(MLP)은 각 뉴런들이 선형모델과 활성함수로 모두 연결된 (fully connected) 구조였습니다.
  - Convolution 연산은 이와 달리 커널(kernel)을 입력벡터 상에서 움직여가 면서 선형모델과 합성함수가 적용되는 구조입니다.
  - Convolution 연산의 수학적인 의미는 신호(signal)를 커널을 이용해 국소적 으로 증폭 또는 감소시켜서 정보를 추출 또는 필터링하는 것입니다.
  - 커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용합니다.
  - Convolution 연산은 1차원뿐만 아니라 다양한 차원에서 계산 가능합니다.

- 2차원 Convolution 연산 이해하기

  - 2D-Conv 연산은 이와 달리 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조입니다.

  - 입력 크기를 (H, W), 커널 크기를 (KH, KW), 출력 크기를 (OH, OW)라 하면 출력 크기는 다음과 같이 계산합니다.

    ![image-20220124003805653](https://user-images.githubusercontent.com/62732145/151790541-8aec4e80-7293-4114-b74f-4d226e5c592e.png)

  - 채널이 여러개인 2차원 입력의 경우 2차원 Convolution을 채널 개수만큼 적용한다고 생각하면 됩니다.

  - 텐서를 직육면체 블록으로 이해하면 좀 더 이 해하기 쉽습니다

- Convolution 연산의 역전파 이해하기

  - Convolution 연산은 커널이 모든 입력데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 됩니다.

### (Python 3-2강) Pythonic code

- split & join
  - split : string type의 값을 “기준값”으로 나눠서 List 형태로 변환
  - join : string으로 구성된 list를 합쳐 하나의 string으로 반환

- list comprehension
  - 기존 List 사용하여 간단히 다른 List를 만드는 기법
  - 포괄적인 List, 포함되는 리스트라는 의미로 사용됨
  - 파이썬에서 가장 많이 사용되는 기법 중 하나
  - 일반적으로 for + append 보다 속도가 빠름
- enumerate & zip
  - enumerate : list의 element를 추출할 때 번호를 붙여서 추출
  - zip : 두 개의 list의 값을 병렬적으로 추출함

- lambda & map & reduce
  - lambda
    - 함수 이름 없이, 함수처럼 쓸 수 있는 익명함수
    - 수학의 람다 대수에서 유래함
    - Python 3부터는 권장하지는 않으나 여전히 많이 쓰임

  - map
    - 두 개 이상의 list에도 적용 가능함, if filter도 사용 가능
    - python3 는 iteration을 생성 -> list을 붙여줘야 list 사용 가능
    - 실행시점의 값을 생성, 메모리 효율적

  - reduce : map function과 달리 list에 똑같은 함수를 적용해서 통합

- iterable objects
  - Sequence형 자료형에서 데이터를 순서대로 추출하는 object
  - 내부적 구현으로 \_\_iter\_\_와 \_\_next\_\_가 사용됨
  - iter()와 next() 함수로 iterable 객체를 iterator object로 사용
- generator
  - iterable object를 특수한 형태로 사용해주는 함수
  - element가 사용되는 시점에 값을 메모리에 반환
    - yield를 사용해 한번에 하나의 element만 반환함
  - generator comprehension
    - list comprehension과 유사한 형태로 generator형태의 list 생성
    - generator expression이라는 이름으로도 부름
    - [] 대신 ()를 사용하여 표현
  - 일반적인 iterator는 generator에 반해 훨씬 큰 메모리 용량 사용
  - list 타입의 데이터를 반환해주는 함수는 generator로 만들어라!
    - 읽기 쉬운 장점, 중간 과정에서 loop 이 중단될 수 있을 때!
  - 큰 데이터를 처리할 때는 generator expression을 고려하라!
    - 데이터가 커도 처리의 어려움이 없음
  - 파일 데이터를 처리할 때도 generator를 쓰자
- passing arguments
  - Keyword arguments : 함수에 입력되는 parameter의 변수명을 사용, arguments를 넘김
  - Default arguments : parameter의 기본 값을 사용, 입력하지 않을 경우 기본값 출력
  - Variable-length arguments
- 가변인자 (variable-length)
  - 개수가 정해지지 않은 변수를 함수의 parameter로 사용하는 법
  - Keyword arguments와 함께, argument 추가가 가능
  - Asterisk(*) 기호를 사용하여 함수의 parameter를 표시함
  - 입력된 값은 tuple type으로 사용할 수 있음
  - 가변인자는 오직 한 개만 맨 마지막 parameter 위치에 사용가능
  - 가변인자는 일반적으로 *args를 변수명으로 사용
  - 기존 parameter 이후에 나오는 값을 tuple로 저장함
- 키워드 가변인자 (Keyword variable-length)
  - Parameter 이름을 따로 지정하지 않고 입력하는 방법
  - asterisk(*) 두개를 사용하여 함수의 parameter를 표시함
  - 입력된 값은 dict type으로 사용할 수 있음
  - 가변인자는 오직 한 개만 기존 가변인자 다음에 사용
- asterisk
  - 흔히 알고 있는 *를 의미함
  - 단순 곱셈, 제곱 연산, 가변 인자 활용 등 다양하게 사용됨
  - unpacking a container
    - tuple, dict 등 자료형에 들어가 있는 값을 unpacking
    - 함수의 입력값, zip 등에 유용하게 사용가능

# 2. 퀴즈 리뷰

### [기본 퀴즈] AI Math 7강 퀴즈 - 통계학 맛보기

1. 표본의 평균 구하기
2. 표본분산 구하기
3. 표본표준편차 구하기
4. 정답 레이블을 one-hot 벡터로 표현한다면 하나의 정답 레이블 벡터의 원소의 개수는 1이 아니다.
5. KL(*P*∥*Q*) 는 KL(*Q*∥*P*) 와 같지 않다.

### [기본 퀴즈] AI Math 8강 퀴즈 - 베이즈 통계학 맛보기

1.  *P*(*A*∩*B*) = *P*(*A*)*P*(*B*∣*A*)
2.  사후확률 (posterior) 은 가능도 (likelihood) 에 반비례하지 않는다.
3.  *P*(*A*∣*B*) = *P*(*A*)*P*(*B*∣*A*) */* *P*(*B*) 
4.  *P*(*A*∣*B*)=*P*(*B*∣*A*)*P*(*A*) */* *P*(*B*∣*A*)*P*(*A*)+*P*(*B*∣¬*A*)*P*(¬*A*)
5.  모든 변수에 대한 조건부 확률만으로 인과관계를 추론할 수 없다.

### [기본 퀴즈] AI Math 9강 퀴즈 - CNN 첫걸음

1. 연속적인 변수에 대한 함수 f, g 사이의 convolution을 나타내는 수식 구하기
2. 입력 벡터와 가중치 벡터가 주어질 때 잠재 변수 값 구하기
3. 입력 벡터와 가중치 벡터가 주어질 때 잠재 변수 값 구하기
4. 입력 행렬과 가중치 행렬이 주어질 때 잠재 변수 값 구하기
5. 입력 행렬과 가중치 행렬이 주어질 때 잠재 변수 값 구하기

# 3. 피어세션 정리

-  수학 공부를 어떻게 해야할까
   - 강의 내용 발표하기
   - 기본 과제 코드 리뷰
   - 심화과제는 매주 월요일 내용 보고

# 4. 멘토링 세션

- 슬랙에 질문하면 답장해주신다.
- 멘토링 시간 활용
  - 현업/대학원 등에서 인공지능 관련 업무/연구에 종사하는 분들
  - 어떻게 공부했는지, 취업했는지, 현업에선 무엇을 중요하게 여기는 지 등등
- 멘토링 질문
  - 현업에서는 AI 이론에 대해서 얼마나 깊게 알아야하나요?
    - 부스트캠프 내용은 다 알아야한다.

  - 학사 이후 부스트캠프 교육을 받은 사람 vs 석/박을 마친 사람들의 대우 차이 (ex 연봉)
    - 케바케다

  - 취업율은? 운영진한테
  - 석사는 꼭 해야하나?
    - 영상보자. 석사의 장점? 

  - 전국적으로 개발자 부족 현상(a.k.a. 웃돈 주고 개발자 모셔간다)이 있다고 하는데, 실제로 그런가요? 그렇다면 개발자는 왜 이렇게 부족한가요?
    - 공급이 부족, 시니어 개발자가 부족하다, 분야도 다양

- 멘토님의 업무
  - 연구 프로젝트 : 진료 보조용 인공지능 소프트웨어 개발.
  - 학회, 논문 : 랭킹 알려주심. 논문을 알고, 코드를 알아야함
  - 현업 프로젝트 
    - 원내 인프라 구축. 서버 설치, 플랫폼 설치. 다 알아야 할 듯
    - 데이터 엔지니어링
  - 프로젝트 소개 : picking robot, 물류배송 자율 주행, 드론 주행
  - 임상의를 위한 인공지능 교육
  - 의료 인공지능 경진대회
  - 의료 인공지능 기업소개 : lunit, vuno
  - 채용공고 : 깃허브, 국립대학병원 연구직

# 5. 마스터 클래스

- 수학을 공부하는 방법

  - 손으로 익히자. 즉, 익숙해지는 것을 목표로

  - 많이 보는 것보단 많이 사용

  - 똑똑하게 익숙해지는 방법 - 많이 보는 것보단 많이 사용하자!

    - 용어의 정의는 외우는 것부터 시작.
      -  교과서나 위키피다아 활용.
      -  헷갈리면 인공지능 커뮤니티에 물어보자 

    - 용어를 외우곤 예제를 찾아보자
      - Ex) likelihood example

- 여러 모델들의 수학적 원리를 이해하고 있어야 하는가?

  - 적어도 원리를 이해하는데 필요한 기초를 갖춰야 한다.

  - 어떤 분야가 기초인가?
    - 선형대수/확률론/통계확
      - 기업 및 대학원 면접 때 많이 물어봄
      - 알고리즘과 최적화 내용도 같이 공부하자
    - 머신러닝 이론을 공부할 게 아니라면 해석학/위상수학은 x
    - 기초 자체를 공부하기보다 머신러닝에서 어떻게 활용되는 지 검색해보자
      - Ex) 분류 문제에서 왜 cross-entrop 를 손실함수로 사용하는가?
      - 즉, 기초에서 끝나는 게 아닌 걔네들을 어떻게 활용하는지 이해해야 완성이다!

- ML 엔지니어는 수학을 어느 정도 알아야 할까?

  - 필요한 걸 공부해서 빠르게 따라잡을 수 있을만큼 알아야 한다. 

  - 즉, 쌓아놓은 기초를 가지고 빨리 따라갈 수 있는 게 중요하다.

  - 수학이 필요할 떈?
    - 문제를 정의하고, 정보를 모아서 솔루션을 구하는데 사용

  2. 인공지능 대학원

- Ai 분야에서 학석박 간의 차이는?

  - 분야마다 전문성이 다르고 대중화되지 않은 영역은 학위 과정이 중요할 수 있다. 

- 인공지능 분야가 대학원이 필수인가?

  - 새로운 분야면 대학원을 추천

- 사전질문

​		Q. 가장 중요한 수학 과목은?

​		A. 아까도 말했지만 선대/확률론/통계학

​		Q. ai 분야 학사, 석사, 박사 차이는? 

​		A. 아까 말함

​		Q. 기업에서 대학원생을 선호하는 이유는? 대학원생과 견줄 만한 스펙을 갖출 수 있는 활동은?

​		A. 새로 쓰는 기술들을 먼저 접한 사람들이니까. 논문 없이 새로운 기술을 공부한 걸 보여줄 수 있는가? 

​		Q. 난이도가 낮은 ai math나 딥러닝 서적 추천해주실 수 있나?

​		A. 쉬운 내용을 찾기보단 수학에 익숙해지는 걸 추천한다.

​		Q. 확률론, 통계론 프로젝트 진행할 때 많이 사용되는가? pytorch를 써가며 이해할 수 있는 영역인가?

​		A. 확률론 통계론은 머신러닝에 사용되는 이론적인 부분. 이 내용을 이해해야 코드, 왜 이 loss function을 사용해야하는 지 이해할 수 있다. 꼭! 구글링 하며 이해해보자.

​		Q. 수학이든 모델이든 line-by-line으로 구현해보는 연습을 계속 해보는 것이 좋은지 혹은 모든 이론의 근간이 되는 기초적인 개념 (GD) 정도만 구현할 수 있어도 충분한가?

​		A. 새로운 내용을 접하게 하는 엔지니어로썬 빨리 팔로우업 할 수 있어야 한다. 엔지니어로써 중요한 스킬을 쌓을 수 있으려면 line-by-line으로 구현보는 연습을 통해 기초를 쌓아야 한다.

​		Q. AI가 잘할 수 있는 것과 잘하지 못한 건가?

​		A. AI보단 기계학습의 영역으로 보면 사람이 할 수 있는 건 왠만하면 잘할 수 있다. 이해, 추론도. 데이터를 쌓을 수 있는 영역이라면 가능할 것이라고 생각한다. 다만. 인공지능이 기계학습이 제일 잘하는 거냐라고 하면 아니다라고 할 것이다. 

​		Q. 마스터의 수학 공부 방법은?

​		A. 하루에 30분 정도는 알고 싶은 내용에 투자. 공부하기보단, 위키피디아, example을 찾는데 30분을 사용

​		Q. 전국에 개발자 부족 현상이 일어나는 이유는?

​		A. 멘토님이 말씀해주신 내용

​		Q. AI 엔지니어로 산업 현장에 나가기 위해 현재 교육 과정 위에 또 어떤 것들을 준비하면 좋은가?

​		A. 꼭! MLOps를 연습해두자.

# 6. 회고

통계학을 제대로 이해하고 지나가야할 것 같습니다. 내일도 화이팅!
