# 1. 1월18일 (Day 2) 학습 정리

### AI math 3강  경사하강법 - 순한맛

미분(differentiation)이란? 변수의 움직임에 따른 **함수값의 변화를 측정하기 위한 도구**로 최적화에서 제일 많이 사용하는 기법이다.

- 미분의 정의 - 변화율의 극한(limit)  

  ![CodeCogsEqn](https://user-images.githubusercontent.com/62732145/149924638-ab8cce08-5aaf-4dc0-a359-953396fe3922.png) 
  
- 미분은 함수 ![1](https://user-images.githubusercontent.com/62732145/149925152-63f25886-36a3-4f25-97fe-06582b6cf349.png)의 주어진 점 ![2](https://user-images.githubusercontent.com/62732145/149925236-5324b6d7-7417-4354-af36-b7e178cb3446.png)에서의 **접선의 기울기**를 구하는 것.

- why 미분? - 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는지 혹은 감소하는지를 알 수 있기 때문이다.

- **경사상승법(gradient ascent)**

![경사상승법](https://user-images.githubusercontent.com/62732145/149925322-1c5f88c0-200a-48d4-bae7-44dc3feb2dba.png)  
  :  미분값을 더하면 경사상승법 이라 하며 함수의 극대값의 위치를 구할 때 사용한다. 목적함수를 최대화할 때 사용한다.

- **경사하강법(gradient descent)**

![경사하강법](https://user-images.githubusercontent.com/62732145/149925339-c7148374-f11f-41aa-aa9f-5d138f9d5682.png)  
  : 미분값을 빼면 경사하강법 이라 하며 함수의 극소값의 위치를 구할 때 사용한다. 목적함수를 최소화할 때 사용한다.

변수가 벡터라면? 벡터가 입력인 다변수 함수의 경우 **편미분(partial differentiation)**을 사용한다.

- 편미분  

  ![CodeCogsEqn (1)](https://user-images.githubusercontent.com/62732145/149926052-c5496e4f-c251-4807-8ff5-00c9541abd3f.png)

- 각 변수 별로 편미분을 계산한 **그레디언트(gradient) 벡터**를 이용하여 경사하강/경사상승법에 사용할 수 있다.  

  ![CodeCogsEqn (2)](https://user-images.githubusercontent.com/62732145/149926065-6d15abda-c31e-4988-87d8-3597b741977c.png)

### AI math 4강  경사하강법 - 매운맛

경사하강법으로 선형회귀 계수 구하기.

- 선형모델의 경우 위와 같이 역행렬을 이용해서 회귀분석이 가능하다.

- 역행렬을 이용하지 말고 경사하강법을 이용해 적절한 선형모델을 찾아보자.

- 선형회귀의 목적식은 ![3](https://user-images.githubusercontent.com/62732145/149925433-fc77b42f-f86f-400f-8768-39397deb53fd.png)이고 이를 최소화하는 ![4](https://user-images.githubusercontent.com/62732145/149925463-115f4dbc-8dfb-4362-b280-94de3b2848a8.png)를 찾아야 하므로 다음과 같은 그레디언트 벡터를 구해야 한다.  

- 이제 목적식을 최소화하는 ![4](https://user-images.githubusercontent.com/62732145/149925628-a94fa607-6249-464f-b4fe-66b1410eb8b4.png)를 구하는 경사하강법 알고리즘은 다음과 같다.  
  
**확률적 경사하강법(stochastic gradient descent)** 이란? 모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부를 활용하여 업데이트하는 기법이다.

- 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않으므로, SGD를 통해 목적식을 최적화할 수 있다.

- SGD는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는데 도움이 된다.

  ![미니배치 연산](https://user-images.githubusercontent.com/62732145/149925772-95742458-e906-408c-a897-c7e17dd5c1d1.png)

- SGD는경사하강법보다 머신러닝 학습에 더 효율적이다.

- 하지만 SGD는 랜덤하게 추출한 일부 데이터를 사용하기에, 학습 중간 과정에서 결과의 진폭이 크고 불안정하지만 속도가 매우 빠르다.

  ![img](https://t1.daumcdn.net/cfile/tistory/996AFC3C5B0CF0C901)

  (출처: https://twinw.tistory.com/247)

  ![img](https://blog.kakaocdn.net/dn/SRACX/btqAkxQ8pkf/clRtX6wjdmVfpU9Z9V1rB1/img.png)

  (출처: https://nonmeyet.tistory.com/entry/Batch-MiniBatch-Stochastic-%EC%A0%95%EC%9D%98%EC%99%80-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EC%98%88%EC%8B%9C)

- 미니배치는 전체 학습데이터를 배치 사이즈로 나누어서 순차적으로 진행하기에, SGD 보다 낮은 오차율을 가지고 Batch 보다 빠르다. 
