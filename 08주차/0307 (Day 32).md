# 0307 (Day 32)

# ****(02강) Annotation Data Efficient Learning****

## 1. Data augmentation

### 1.1 Learning representation of dataset

- 뉴럴 네트워크는 data를 컴퓨터로만 이해할 수 있는 지식의 형태로 녹여넣는 (압축하는) 모델, 데이터를 통해 패턴을 분석하는 거라 모델이 데이터를 편식하지 않고 머금는 게 best
- 그러나 언제나 데이터셋은 biased 돼있다. training data 는 실제 data가 아니다
- 학습은 training data 는 real data의 극히 일부인 dataset. → 못본 data가 너무 많아진다.
- bias가 문제인 이유:
- ex) 밝은 영상의 데이터만 주어지고, 모델을 학습했을때, 모델이 어두운 이미지를 받으면 인식할 수 없다.
- 이러한 sample data와 real data의 gap을 줄일 수 있는 방법? → data augmentation
- data augmentation : 학습 데이터를 불리는 과정
- 영상의 밝기 바꾸기
    - 일정 숫자를 더해준다. 255를 넘지 않게
    - scaling
- 회전하기
- 크롭
    - 간단하면서도 강력. 중요한 학습 방법에 집중
    - indexing
- 기하학 변화
    - affine transformation (share)
        - 변환 전후에도 선은 선으로, 길이의 비율 일정하게 유지. 평행관계가 유지하는 tansform
        - 대응하는 점 3개를 mapping하는 대응쌍을 함수에 준다.
- cutmix
    - 일부를 잘라 합성
    - 라벨도 비율에 따라 합성하는 것이 key point
    - 효과가 좋음
- randAugment
    - 이러한 기법(poolicy)들을 조합해서 전혀 다른 데이터를 생성
    - 어떤 조합이 더 성능이 잘 나올까? 어떤 순서로 써야할까?
    - 를 답변. 랜덤하게 수행해보고 성능이 가장 좋은 거 선택
    - 예시
        - 어떤 augmentation을 사용할지
        - 얼마나 강하게 사용할지
        - 

## 2. Leveraging pre-trained information

### 2.1 Transfer learning

- 기존에 학습된 사전 지식을 활용해, 연관된 새로운 task에 적은 노력으로도 높은 성능에 도달할 수 있는 기술
- 한 데이터셋에서 배운 지식을 다른 데이터셋에서 활용
- approach 1:
    - 미리 학습한 모델을 준비
    - 마지막 fc layer 대신 새로운 fc layer을 붙힌다.
    - 새로 붙힌 layer만 새로운 task를 등록시키고, 그 데이터셋에 대해 학습시킨다
    - 위에 convolution layer는 freeze
    - 데이터가 정말 작을때
- approach 2:
    - 첫번째 방법과 달리 convolution layer도 learning rate를 낮게 잡아 학습 시킴.
    - FC layer는 learning rate를 높게
    - 데이터가 첫번째 보단 많을때

### 2.2 Knowledge distillation

- 이미 훈련된 선생 모델 (큰 모델)을 학생 모델 (작은 모델)에 주입해 학습
- 모델 압축에 사용됨
- 최근, 선생 모델에서 생성된 출력을 unlabeled 된 data를 pseudo-label로 자동 생성하는 pseudo-labeling에 사용
- 이후, 학생 모델을 사용할때도 정규화에 사용할 수 있음.
- 선생-학생 네트워크 구조
    - (훈련된) 선생 모델의 결과와 (훈련되지 않은) 학생 모델의 결과를 KL div. Loss를 계산해, 학생 모델만 역전파를 통해 학습시킨다.
    - 학생 모델이 선생 모델을 따르게 함
    - 라벨링을 전혀하지 않음 → unsupervied learning
    - 임의의 데이터 사용 가능
- hard label (one-hot vector) vs soft label
- softmax with temperature(T)
    - 소프트맥스만 사용하면 값 차이가 극단적으로 치우칠 수 있음. 그래서 T로 나눠줘서 중간값으로 만들어줌
- Semantic information is not considered in distillation
    - soft label로 나온 전체 값들이 어떤 모습을 띄고 있는지, 그래서 선생 모델과 비슷한 방향으로 학습하는 것이 중요하지, 하나하나의 값들은 중요하지 않다.
- distillation loss
    - KLdiv(Soft label, Soft prediction)
    - 선생과 학생의 예측값 차이를 measure하는 Loss
    - 그럼으로써 선생이 알고있는 걸 따라하게 한다.
- student loss
    - CrossEntropy(Hard label, Soft prediction)
    - 학생이 출력한 것과 true label이 일치하도록 하는 Loss
    - 맞는 값을 찾게끔 한다.
- 두 loss를 사용해 student 모델만 학생하게끔

## 3. Leveraging unlabeled dataset for training

### 3.1 Semi-supervised learning

- unlabeled면 온라인 상의 데이터 활용 가능
- Semi-supervised learning : 많은 양의 unlabel과 적은 양의 label 활용
- label 데이터로 모델 학습 → unlabel 데이터를 이 모델로 pseudo-labeling → pseudo-labeled 된 데이터와 이전 label 데이터로 다시 학습

### 3.2 Self-training

- Augmentation + Teacher-Student networks + semi-supervised learning
1. imageNet으로 라벨된 데이터로 선생 모델 학습
2. 선생 모델로 unlabeled data 라벨링
3. 라벨 데이터와 수도 라벨 데이터를 randaugment를 사용해 학생 모델 학습
4. 학습된 학생모델을 새로운 선생 모델로 사용
5. 반복 
- 이전과는 다르게 학생 모델이 점점 커짐

# ****(03강) Image Classification 2****

### 1.1 Going deeper with convolutions

- 네트워크가 깊어질수록 더 복잡한 관계에 학습이 가능하고, 더 큰 receptive fields 즉, 더 많은 주변을 참조해 신중히 결론을 내릴 수 있다.
- 근데 이게 정말 참인가?

### 1.2 Hard to optimize

- 깊게 쌓을수록 gradient가 accumulate 되어, gradient vanishing/exploding 발생
- 계산 복잡도가 올라가, 더 좋은 성능의 컴퓨터가 필요
- Degradation 문제 발생

## 2. CNN architectures for image classification 2

### 2.1 GoogLeNet

- inception module
    - 하나의 layer에 다양한 convolution (수평확장)
    - 계산 복잡도 상승 → 1x1 conv으로 채널 차원 감소
- 1x1 convolution
    - 1x1 conv의 필터 수만큼 내적 → 필터 수만큼 출력
    - 공간 크기가 아닌 채널 수만
- 전체적인 구조
    - 일반적인 cNN
    - inception module 쌓기
    - gradient vanishing 해결을 위해 중간중간 auxiliary classifier 투입 → loss 측정
    - fc layer, softmax score
- Auxiliary classifiers
    - softmax를 사용해 gradient vanishing이 일어나지 않게 중간중간 loss measure
    - 2개의 fc layer과 1개의 1x1 (=FC)
    - 학습 도중에만 사용, test엔 x

## 2.2 ResNet

- 네트워크의 depth 가 성능에 중요하다는 걸 시사
- degradation problem
    - layer가 클수록 training, test error 둘 다 큼
    - overfitting 이었으면 training error에선 적은 layer보다 작았어야함
    - 결국 최적화 문제다 (gradient vanishing/exploding)
- 연구 가설
    - mapping을 학습할때 높은 층을 쌓아서 곧바로 x에서 H(x)에 관계를 학습하려, 찾으려 하면 복잡하기 때문에 학습에 어려움
    - 한 번에 찾지말고, 현재 주어진 identity외에 잔여부분만 학습하게 하면 학습의 부담을 줄여줌
    - h(x)가 복잡할때 적어도 자기 자신을 보존할 필요 없게 한다.
- 해결책 : Shortcut connection
    - 역전파를 identity mapping도 해줘 gradient vanishing 해결
- 성능이 좋은 이유?
    - n개의 layer일때, 역전파의 경우의 수를 모두 따지면 $2^n$저전
- 전체적인 구조
    - 7 x 7, he initialization
        - why use? identity connection으로 출력된 2개의 activation 결과값이 더해지면, 값이 굉장이 커지게 되므로
    - 3 x 3, 수많은 residual block
        - convolution block이 나눠져서, 한 단계를 넘어갈때마다 공간해상도는 절반으로, 채널 수는 2배씩
    - 1 개의 FC Layer

### 2.3 Beyond ResNets

- DenseNet
    - Dense block은 채널 축으로 concatenated, resnet은 +
        - concatenate : 합치지만 각 신호들을 보존하고 있음
        - + : 신호를 합쳐버림
    - 바로 이전이 아닌 훨씬 이전의 정보들도 넘겨줌
    - 모든 layer들이 연결되어 있음 (Dense)
- SENet
    - 현재 주어진 채널들의 activation들을 명확하게 할 수 있도록 모델링
    - attention
        - squeeze : 각 채널의 공간 정보를 없애고, 분포도 (magnitude)만
        - excitation : FC를 통해 채널의 연관성을 고려1
- EfficientNet
    - 기존의 설계 방식 성능을 높이는 방법
        - width scailing : 채널 축 넓게 (수평확장), inception module과 유사, googlenet, densenet
        - depth scailing : layer을 더 많이, resenet
        - resolution scailing : input resolution을 더 크게 (?)
    - 성능의 증가폭이 각각 다름
    - 적절한 비율폭으로 동시에 scailing하면 더 효과가 좋지 않을까 → compound scailing
- irregular convolution
    - 사용하는 이유 : 사람이나 동물의 상대적인 신체를 고려하여 고안
    - deformable convolution
    - standard CNN 와 2D offsets 내적

## 3. Summary of image classification

### 3.1 Summary of image classification

- AlexNet : 가장 간단한 CNN, 모델의 사이즈는 크므로 방대, 성능이 낮다
- VGGNet : 3x3, 가장 느린 계산 속도, 모델 사이즈도 크다
- GoogleNet : inception module,
- ResNet : vgg보단 작은 계산량, 메모리 사이즈, inception 계열보단 크고 느림

### 3.2 CNN backbones

- googlnet이 사이즈 측면에서 봤을 땐 효율적임. 그러나 구현이 어려움
- 그래서 backbone vgg, res 사용. feature map을 만드는데 많이 사용
