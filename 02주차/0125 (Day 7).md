# 1. 1월 25일 (Day 7) 강의 정리

### (04강) AutoGrad & Optimizer

- layer

  - transformer, resnet

  - resNet : softmax, Linear, normalization, feed forward. 왼쪽 encoder, 오른쪽 decoder

- torch.nn.Module

  - 딥러닝을 구성하는 Layer의 base class
  - Input, Output, Forward, Backward 정의
    - Backward때는 Autograd가 되기에 학습의 대상은 weights가 된다.
  - 학습의 대상이 되는 parameter(tensor) 정의

- nn.Parameter

  - Tensor 객체의 상속 객체
  - nn.Module 내에 <span style="color:red">attribute가 될 때는 required_grad=True</span>로 지정되어 학습 대상이 되는 Tensor 
  - 대부분의 layer에는 weights 값들이 지정되어 있어 우리가 직접 지정할 일은 잘 없음
  - vs tensor
    - tensor로 하면 output은 동일하지만 layer마다 parameters를 저장하지 않기에 값을 확인할 수 없다.

- Backward

  - Layer에 있는 Parameter들의 미분을 수행
  - Forward의 결과값 (model의 output=예측치)과 실제값간의 차이(loss) 에 대해 미분을 수행 
  - 해당 값으로 Parameter 업데이트

- Backward from the scratch

  - 실제 backward는 Module 단계에서 직접 지정가능하지만 할 필욘 없음
  - 지정하면 Module에서 backward 와 optimizer 오버라이딩
  - 사용자가 직접 미분 수식을 써야하는 부담이 있다.
    - 쓸일은 없으나 순서는 이해할 필요는 있음

**Further Question**

1. epoch에서 이뤄지는 모델 학습 과정을 정리해보고 성능을 올리기 위해서 어떤 부분을 먼저 고려하면 좋을지 같이 논의해보세요
2. optimizer.zero_grad()를 안하면 어떤 일이 일어날지 그리고 매 batch step마다 항상 필요한지 같이 논의해보세요

###  (05강) Dataset & Dataloader

- 모델에 데이터를 먹이는 방법

  - collecting, cleaning, pre processing : 데이터 모으기

  - Dataset : 그 데이터셋의 시작, 길이, geitem (map-style, 하나의 데이터를 불러올 때 인덱스를 사용해서 어떻게 반환할지) 선언
  - transforms : 이미지 전처리, ToTensor() (data argumentation할 때 데이터 tensor로 바꿔줌) 
    - 파이토치에서 데이터셋을 전처리 (이미지), 데이터셋을 tensor (숫자)는 구분이 됨

  - Dataloader : 데이터를 묶어서 feeding 해줌. batch, shuffe

- Dataset 클래스
  - 데이터 입력 형태를 정의하는 클래스
  - 데이터를 입력하는 방식의 표준화
  - Image, Text, Audio 등에 따른 다른 입력정의
- Dataset 클래스 생성시 유의점
  - 데이터 형태에 따라 각 함수를 다르게 정의함
  - 모든 것을 데이터 생성 시점에 처리할 필요는 없음 : image의 Tensor 변화는 학습에 필요한 시점에 변환. 병렬적으로 수행 
  - 데이터 셋에 대한 표준화된 처리방법 제공 필요
  - 최근에는 HuggingFace등 표준화된 라이브러리 사용
- DataLoader 클래스
  - Data의 Batch를 생성해주는 클래스
  - 학습직전(GPU feed전) 데이터의 변환을 책임
  - Tensor로 변환 + Batch 처리가 메인 업무
  - 병렬적인 데이터 전처리 코드의 고민 필요
  - sampler, batch_sampler : https://subinium.github.io/pytorch-dataloader/
  - collate_fn
    - [[D, L], [D, L], ...] -> [[D D D], [L, L, L]]. 
    - 언제 사용하는가? text 처리할 때, 길이가 다른 글자수에 대해 fadding해줄 때 사용

**Further Question**

1. DataLoader에서 사용할 수 있는 각 sampler들을 언제 사용하면 좋을지 같이 논의해보세요!

   sampler는 index를 컨트롤하는 방법입니다. 데이터의 index를 원하는 방식대로 조정합니다. 즉 index를 컨트롤하기 때문에 설정하고 싶다면 `shuffle` 파라미터는 `False`(기본값)여야 합니다.

   - `SequentialSampler` : 항상 같은 순서
   - `RandomSampler` : 랜덤, replacemetn 여부 선택 가능, 개수 선택 가능
   - `SubsetRandomSampler` : 랜덤 리스트, 위와 두 조건 불가능
   - `WeigthRandomSampler` : 가중치에 따른 확률
   - `BatchSampler` : batch단위로 sampling 가능
   - `DistributedSampler` : 분산처리 (`torch.nn.parallel.DistributedDataParallel`과 함께 사용)

   (출처 : https://subinium.github.io/pytorch-dataloader/)

2. 데이터의 크기가 너무 커서 메모리에 한번에 올릴 수가 없을 때 Dataset에서 어떻게 데이터를 불러오는게 좋을지 같이 논의해보세요!

# 2. 퀴즈 리뷰

### [퀴즈] PyTorch 구조 학습하기

1. backward라는 함수가 호출되면서 해당 값으로 파라미터가 업데이트해가는 과정입니다. epoch가 돌아갈 때마다 순서대로 연산이 진행이 되는데, 연산의 진행 과정은?

   optimizer.zero_grad()

   loss = criterion(outputs, labels)

   loss.backward()

   optimizer.step()

2. DataLoader의 아래 옵션 중 데이터를 어떻게 뽑을지 index를 정해주는 옵션은?

   DataLoader(**dataset**, **batch_size**=1, **shuffle**=False, **sampler**=None, **batch_sampler**=None, **num_workers**=0, **collate_fn** = None, **pin_memory**=False, **drop_last**=False, **timeout** = 0, **worker_init_fn**=None, *, **prefetch_factor**=2, **persistent_workers**=False)

   : sampler	


# 3. 과제 리뷰

### [기본 과제] Custom Dataset 과제

# 4. 피어세션 정리

- 강의 리뷰 및 QnA
  - (04강) AutoGrad & Optimizer
  - (05강) Dataset & Dataloader
- 퀴즈 리뷰
  - [퀴즈] PyTorch 구조 학습하기
- 과제 리뷰
  - [기본 과제] Custom Model 개발하기

# 5. 회고

Custom Model을 복습하며 완전히 숙지해보겠습니다. 내일도 화이팅!
