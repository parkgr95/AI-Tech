# 1. 1월 26일 (Day 8) 강의 정리

### (06강)  모델 불러오기

- model.save()

  - 학습의 결과를 저장하기 위한 함수

  - 모델 형태(architecture)와 파라메터를 저장.

    - 통상적으로 파라메터만 저장. 코드를 공개하고 싶지 않을 때

  - 모델 학습 중간 과정의 저장을 통해 최선의 결과모델을 선택

  - 만들어진 모델을 외부 연구자와 공유하여 학습 재연성 향상

    ```python
    # Print model's state_dict
    print("Model's state_dict:")
     # state_dict : 모델의 파라메터를 표시
    for param_tensor in model.state_dict(): 
    	print(param_tensor, "\t", model.state_dict()[param_tensor].size())
        
    torch.save(model.state_dict(), # 모델의 파라메터를 저장. OrderedDict 타입으로
               os.path.join(MODEL_PATH, "model.pt")) # pt 확장자로 저장
    
    new_model = TheModelClass() 
    new_model.load_state_dict(torch.load(os.path.join(
        MODEL_PATH, "model.pt"))) # "같은 모델"의 형태에서 파라메터만 load
    
    # 모델의 architecture와 함께 저장
    torch.save(model, os.path.join(MODEL_PATH, "model.pt"))
    # 모델의 architecture와 함께 load
    model = torch.load(os.path.join(MODEL_PATH, "model.pt"))
    
    ```

- checkpoints

  - 학습의 중간 결과를 저장하여 최선의 결과를 선택

  - earlystopping 기법 사용시 이전 학습의 결과물을 저장

  - loss와 metric 값을 지속적으로 확인 저장

  - 일반적으로 epoch, loss, metric을 함께 저장하여 확인

  - colab에서 지속적인 학습을 위해 필요

    ```python
    torch.save({ # 모델의 정보를 epoch단위로 저장
            'epoch': e,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': epoch_loss,
            },
    f"saved/checkpoint_model_{e}_{epoch_loss/len(dataloader)}_{epoch_acc/len(dataloader)}.pt")
    
    checkpoint = torch.load(PATH)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    
    ```

  - 입맛에 맞게 checkpoints를 사용해 원하는 데이터만 저장하고 보자

- Transfer learning

  - 다른 데이터셋으로 만든 모델을 현재 데이터에 적용

    - ImageNet으로 만든 모델을 dogsandcats에 적용해보고 싶다 할때

  - 일반적으로 대용량 데이터셋으로 만들어진 모델의 성능 ↑

  - 현재의 DL에서는 가장 일반적인 학습 기법

  - backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습을 수행함

    - Freezing : pretrained model을 활용시 모델의 일부분을 frozen 시킴

    ```python
    vgg = models.vgg16(pretrained=True).to(device)
    
    class MyNewNet(nn.Module): # 나만의 모델을 만든다
        def __init__(self):
            super(MyNewNet, self).__init__()
            self.vgg19 = models.vgg19(pretrained=True)
            self.linear_layers = nn.Linear(1000, 1) # 모델에 마지막 Linear Layer 추가
            
        # Defining the forward pass
        def forward(self, x):
            x = self.vgg19(x)
            return self.linear_layers(x)
        
    for param in my_model.parameters():
    	param.requires_grad = False # 원하는 Layer를 제외하고 frozen
    for param in my_model.linear_layers.parameters():
    	param.requires_grad = True
    
    ```

### (07강) Monitoring tools for PyTorch

- Tensorboard

  - TensorFlow의 프로젝트로 만들어진 시각화 도구

  - 학습 그래프, metric, 학습 결과의 시각화 지원

  - PyTorch도 연결 가능 → DL 시각화 핵심 도구

  - scalar : metric 등 상수 값의 연속(epoch)을 표시

  - graph : 모델의 computational graph 표시

  - histogram : weight 등 값의 분포를 표현

  - Image : 예측 값과 실제 값을 비교 표시

  - mesh : 3d 형태의 데이터를 표현하는 도구

    ```python
    import os
    logs_base_dir = "logs"
    os.makedirs(logs_base_dir, exist_ok=True) # Tensorboard 기록을 위한 directory 생성
    
    from torch.utils.tensorboard import SummaryWriter # 기록 생성 객체 SummaryWriter 생성
    import numpy as np
    
    writer = SummaryWriter(logs_base_dir) # 위치만 지정하면 된다.
    for n_iter in range(100):
        '''
        add_scalar 함수 : scalar 값을 기록
    	Loss/train : loss category에 train 값. 실험프로젝트 위치 /이름 
    	n_iter : x 축의 값. iteration
        '''
    	writer.add_scalar('Loss/train', np.random.random(), n_iter)
    	writer.add_scalar('Loss/test', np.random.random(), n_iter)
    	writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    	writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
    writer.flush() # 값 기록 (disk에 쓰기)
    
    %load_ext tensorboard # jupyter 상에서 tensorboard 수행
    %tensorboard --logdir {logs_base_dir} # 파일 위치 지정 (logs_base_dir) 같은 명령어를 콘솔에서도 사용가능. 선호
    ```

- weight & biases

  - 머신러닝 실험을 원활히 지원하기 위한 상용도구

  - 협업, code versioning, 실험 결과 기록 등 제공

  - MLOps의 대표적인 툴로 저변 확대 중

  - 가입후 API 키 확인

  - 새로운 프로젝트 생성하기 (이름 기억 필요)

    ```python
    !pip install wandb -q
    
    config={"epochs": EPOCHS, "batch_size": BATCH_SIZE, "learning_rate" : LEARNING_RATE} # config 설정
    wandb.init(project="my-test-project", config=config)
    # wandb.config.batch_size = BATCH_SIZE
    # wandb.config.learning_rate = LEARNING_RATE
    
    for e in range(1, EPOCHS+1):
        epoch_loss = 0
        epoch_acc = 0
        for X_batch, y_batch in train_dataset:
        	X_batch, y_batch = X_batch.to(device), 
           	y_batch.to(device).type(torch.cuda.FloatTensor)
            # …
            optimizer.step()
            # …
            
    wandb.log({'accuracy': train_acc, 'loss': train_loss}) # 기록 add_~~~ 함수와 동일
    ```

    

# 2. 퀴즈 리뷰

### [퀴즈] 

1. PyTorch의 Checkpoint가 save되었다고 가정할 때, 현재 Checkpoint에서 저장되었던 epoch에서의 Model과 Optimizer를 다시 load하는 코드

   model.load_state_dict(checkpoint['model_state_dict'])
   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

2. TensorBoard : TensorFlow의 프로젝트로 만들어진 시각화 도구로 학습 그래프나 결과에 대한 시각화를 지원하며 최근에는 PyTorch와의 연결도 가능하게 해주는 도구 혹은 라이브러리

# 4. 피어세션 정리

# 5. 회고

