# 1. 2월 8일 (Day 14) 강의 정리

### (04강) Convolution은 무엇인가?

- **Convolution**

  ![image-20220208203541618](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208203541618.png)

  - 두 함수가 있을 때, 두 함수를 잘 섞어주는 방법 또는 operator로 정의한다.

  ![image-20220208203659971](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208203659971.png)

  - output (feature map) = filter 와 Image의 합성곱

  - **2D convolution in action**

    ![image-20220208203758299](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208203758299.png)

    - 적용하고자 하는 filter 모양에 따라 convolution output이 바뀐다.

  - **RGB Image Convolution**

    ![image-20220208204957423](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208204957423.png)

    - 32 x 32 x 3 (RGB) Image를 convolution 한다고 가정하면, filter의 크기는 (image 와) 같다고 가정되기 때문에  5 x 5 x **3**짜리 convolution filter (kernel)를 사용하게 된다.

    ![image-20220208205115476](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208205115476.png)

    - 32 x 32 x 3 image에 5 x 5 x 3 filter를 적용하게 되면 output의 channel은 1이다. 그런 convolution filter가 4개가 있다면, output 역시 28 x 28 x 4 feature가 된다.

    - input channel과 output (convolution feature map)의 channel을 알면, 여기에 적용되는 convolution feature의 크기 역시 계산할 수 있다!

  - **Stack of Convolutions**

    ![image-20220208210324376](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208210324376.png)

    - convolution을 여러 번 (stack)할 수도 있다. 이 때, 한 번 convolution을 거친 후에는 각 element 별로 (non-linear) activation function을 통과하게 된다.

    - **!! 연산에 필요한 parameter의 숫자를 잘 생각해야 한다**

      - 처음에 필요한 parameter 수 = 5 x 5 x 3 x 4

        : 5 x 5  (kernel의 size) x 3 (input channel 수) x 4(output channel 수)

      - 그 다음 필요한 parameter 수 = 5 x 5 x 4 x 10

        : 5 x 5 x 4 (현재 convolution하는 feature의 demension) x 10

- **Convolutional Neural Network**

  ![image-20220208212630055](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208212630055.png)

  - CNN = convolution layer, pooling layer, fully connected layer (FC layer, dense layer)

    - Convolution and pooling layers : feature extraction.

    - Fully connected layer : descision making Ex) classification

  - 최근, FC layer가 점점 없어지거나 최소화 되는 추세이다.
    머신러닝에서 학습하고자 하는 어떤 모델의 parameter의 숫자가 늘어날수록 학습이 어렵고 generalization performance가 떨어진다. 즉, 아무리 학습을 잘 시켜도, 실생활에서 deploy하면 성능이 나오지 않는다. 따라서, CNN의 발전 방향은 같은 모델을 만들고, 최대한 모델을 deep하게 (convolution layer를 최대한 많이) 쌓음과 동시에 parameter 숫자를 줄이는 데 집중하게 된다.
  - **!! 새로운 neural network를 보았을 때, 네트워크의 layer별로 몇 개의 parameter로 이루어져 있고, 전체 parameter 숫자가 몇개인지 생각하는 것이 중요햐다.**

- **Convolution Arithmetic (of GoogLeNet)**

  ![image-20220208213610438](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208213610438.png)

  - 어떤 neural network에 대해, parameter가 몇 ㄴ개인지 직접 계산해 보고 표 값과 비교해보자

- **Stride**

  ![image-20220208213713176](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208213713176.png)

  - convolution filter를 얼마나 자주, 얼마나 dense(sparse) 하게 찍을것인가 ?
  - Stride = 1 : 매 픽셀마다 찍는다. 찍고 1칸 이동. output 2개 줄어듬
  - Stride = 2 : 찍고 2칸 이동. output 4개 줄어듬

- **Padding**

  ![image-20220208214711096](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208214711096.png)

  - 가장자리에 값을 덧대서 끝 값도 convolution operatIon 할 수 있게 한다.

- **Stride ? Padding?**

  ![image-20220208215012366](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208215012366.png)

  1.  4 x 4 -> 2 x 2
  2. input dim = output dim
  3. 5 x 5 -> 2 x 2
  4. 5 x 5 -> 3 x 3

  - input demension + padding x 2 = kernel + stride x (output demension - 1)

  - 원하는 출력값에 맞춰 padding과 stride를 주자

- Convolution Arithmetic

  ![image-20220208215321650](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208215321650.png)

  - what is the numer of parameters of this mode? 3 x 3 x 128 x 64 = 73,728
  - **padding과 stride는 parameter 숫자와는 무관하다.**

- Exercise (of AlexNet)

  - 당시 사용하던 GPU 메모리가 크지 않아서, 2개의 GPU로 나눠서 training

  ![img](https://user-images.githubusercontent.com/45934191/129022821-69d34f40-09d6-4542-961d-32988d24b656.png)

  1. 11 × 11 × 3 × 48 ∗ 2 ≈ 35*k*
  2. 5 × 5 × 48 × 128 ∗ 2 ≈ 307*k* : filter의 크기가 줄어들고, channel이 커짐
  3. 3 × 3 × 128 ∗ 2 × 192 ∗ 2 ≈ 884*k*
  4. 3 × 3 × 192 × 192 ∗ 2 ≈ 663*k*
  5. 3 × 3 × 192 × 128 ∗ 2 ≈ 442*k*
  6. 13 ∗ 13 ∗ 128 ∗ 2 × 2048 ∗ 2 ≈ 177*M* 
  7. 2048 ∗ 2 × 2048 ∗ 2 ≈ 16*M*
  8. 2048 ∗ 2 × 1000 ≈ 4*M*

  - 1\.~ 5. : convolution layer

  - 6\. ~ 8. : dense layer. 각각의 커널이 모든 위치에 대해 동일하게 적용되기 때문에 parameter 수가 매우 많다

  - 대부분의 parameter가 FC layer에 속해 있기 때문에, 뒷 단의 FC layer를 최대한 줄이고, 앞 단의 convolution layer를 깊게 쌓는 것이 네트워크가 발전하는 오늘날의 trend이다.

    depth ↑ parameter 수↓

- **!! 1X1 Convolution**

  ![image-20220208221312813](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208221312813.png)

- 이미지에서 한 픽셀만 보는 것
- Why?
  - Dimension reduction. 채널을 의미.
  - convolution layer를 더 깊게 쌓으며 (depth ↑) parameter 수를 줄일 수 있게 된다

### (05강) Modern CNN - 1x1 convolution의 중요성

- Modern CNN, 1x1 convolution의 중요성

  ## ILSVRC(ImageNet Large-Scale Visual Recognition Challenge)

  ![img](https://user-images.githubusercontent.com/45934191/129125387-11ae7bd8-e051-48d2-bf7e-065d1ebdacb6.png)

  ## AlexNet(8 layers, 60M)

  ![img](https://user-images.githubusercontent.com/45934191/129130544-1cc16bb8-4e03-4d3a-b6a6-f7732d6fe26e.png)
  **5 conv layer, 3 dense layer (8단)**

  당시 GPU가 부족했기 때문에, 두 GPU에 따로 training해 네트워크가 두 개로 나뉘어져 있다.
  `11*11` filter(input) 사용시 하나의 conv 커널이 볼 수 있는 이미지 레벨에서의 영역은 커지지만(receptive field), 상대적으로 더 많은 params가 필요해진다

  #### 성공이유

  - ReLu

    ![img](https://user-images.githubusercontent.com/45934191/129130599-3d70e370-fc86-4a89-844c-94e92ae00ef2.png)

    - activation function이 가져야 하는 중요한 성질 : nonlinear
    - 마지막 scope가 1이기 때문에 네트워크 깊게 쌓았을 때 망치게 하는 성질이 별로 없고, linear 모델이 가지고 있는 좋은 성질을 가지고 있다
    - easy to optimize with GD
    - 0을 기점으로 값이 커지면 slope가 줄어들게 되는데(gradient) -> 내가 가지고 있는 neural network 값이 많이 크면(0에서 많이 벗어나면) 그 곳에서 나오는 gradient가 0에 가까워져 vanishing gradient problem이 생기게 되는데, Relu는 이런 문제를 없애준다.

  - GPU 사용

  - LRN : 입력공간에서, response가 많이 나오는 몇 개를 죽이는 것. 최종적으로 sparse한 activation 나오게 된다

  - Data augmentation

  - Dropout

  지금 시점에서 보면, gpu, activation function Relu, Data augumentation, Dropout 은 당연한 것이지만, 그 당시에는 당연하지 않았다. 지금의 **일반적으로 잘되는 기준**을 잡아준 네트워크라고 볼 수 있다.

  ## VGGNet (19 layers, 110M)

  ![img](https://user-images.githubusercontent.com/45934191/129130671-3289c986-5e0d-446b-88db-9578ded5a663.png)

  - `3*3` conv filter만 사용(중요) : conv filter 크기가 커지면 하나의 conv filter가 찍을때 고려되는 input 크기가 커진다. (receptive filter)

  - Dropout

  - VGG 16, 19

    #### 왜 `3*3` convolution filter를 사용했을까?

  ![img](https://user-images.githubusercontent.com/45934191/129130994-97eaead1-1e22-4b8f-9e35-f2069d7559dd.png)

  ![img](https://user-images.githubusercontent.com/45934191/129130793-6305b87c-26c7-4ab5-a46f-e8356007539a.png)

  마지막 레이어 하나의 값은 , input layer `5*5`의 픽셀이 합해진 값이 된다. receptive field 입장에서는 `3*3`X2 와 `5*5`X1 이 같지만, parameter 수는 전자가 훨씬 적어지게 된다.

  앞으로 나올 다른 네트워크들도 비슷한 이유로 convolution filter의 크기는 대부분 `7*7`을 거의 벗어나지 않는다.

  ## GoogLeNet (22 layers, 4M)

  `1*1` convolution 은 dimension(channel) reduction 효과가 있다. conv feature map이 special dimension 텐서의 depth 방향으로 있는 채널을 줄이기 때문에, `1*1`convolution 잘 활용하면 param 수를 줄일 수 있게 된다.
  `11*11`, `7*7`, 등의 conv보다, `3*3` 을 여러번 활용하는 것이 좋다

  ![img](https://user-images.githubusercontent.com/45934191/129132569-4f14c6bc-d511-470f-a8d7-6533a3f979d0.png)

  - 22 layers
  - NIN(Network in Network) 구조 : 네트워크 모양이 네트워크 안에 있는 구조. 비슷하게 보이는 네트워크가 반복된다.

  ![img](https://user-images.githubusercontent.com/45934191/129132649-8d44d4c9-2169-4900-9b93-7b665af3b29b.png)

  - Inception blocks : 하나의 입력이 들어왔을 때, 여러 개가 퍼졌다가 하나로 합해진다. 각각의 path를 보면,

     

    ```
    1*1
    ```

    convolution 후

     

    ```
    3*3
    ```

    ,

     

    ```
    5*5
    ```

    .

    - 하나의 입력에 대해, 여러 개의 receptive field를 갖는 필터를 거치고, 이것을 통해 여러 응답을 concat하는 효과도 있지만, `1*1`이 중간에 들어가면서, 전체적인 네트워크 params가 줄어든다.

  ### `1*1`convolution 이 왜 params를 줄일까?

  ![img](https://user-images.githubusercontent.com/45934191/129134702-082bd92f-219f-4c8a-a118-18d7e34c3f91.png)

  ## ResNet

  > params 많으면 어떤 문제가 생길까?
  >
  > - overfitting : training error 줄어들지만, testing error는 커진다
  > - 하지만.. Deeper Neural Network도 학습하기 힘들다
  >
  > ![img](https://user-images.githubusercontent.com/45934191/129135344-902c9f4c-d394-48f5-be46-6b8ccc1ea2dd.png)
  >
  > - training error, testing error가 같이 줄어들긴 하지만, training error가 더 작아져도 test error가 더 커서 더이상 학습이 되지 않는 경우가 발생한다.
  >   **-> ResNet : Identity map 추가**

  ![img](https://user-images.githubusercontent.com/45934191/129135533-c63de083-f369-4ed7-82bf-76909c5404dd.png)

  skip connection : input이 밑에서, 위로 올라갈때 neural network의 출력값에 더해주는 것 -> convolution layer가 **`차이`**만 학습
  ![img](https://user-images.githubusercontent.com/45934191/129135753-d3863580-776b-4a2d-9a22-1ed1dfb0c8f8.png)

  **네트워크를 더 깊게 쌓을 수 있는 가능성을 보여준다!**

  ![img](https://user-images.githubusercontent.com/45934191/129135849-91f8ef98-e7bd-44c2-8981-bd16ab61c36e.png)

  x를 더해주는 구조는 같지만, 더해주려면 차원이 같아야 한다.
  Projected Shortcut : 차원을 맞추어 주기 위해 `1*1` convolution으로 채널을 바꾸어 준다. (but 일반적으로는 Simple Shortcut을 더 많이 사용한다.)

  순서에 대한 논쟁은 있지만 원 논문에서는 **BN**이 Convolution 뒤에 일어나는 구조이다.

  ![img](https://user-images.githubusercontent.com/45934191/129136183-ab423526-4ac1-453d-b998-93f328bd0afc.png)

  `3*3`conv 전에 input 채널을 줄이고, conv 후 채널을 다시 늘여 output과 차원을 맞춘다.

  ![img](https://user-images.githubusercontent.com/45934191/129136344-72ec08b6-d64a-4cd4-80e1-4262b21a66b5.png)

  ## DenseNet

  ![img](https://user-images.githubusercontent.com/45934191/129149756-776ff74e-6d3f-4af7-9489-8d9ac5b7c3ed.png)

  convolution 을 통해 나오는 값을 더해준다. 이 때 값을 섞이게 더하는 것이 아니라 concat하는 구조이다.

  ![img](https://user-images.githubusercontent.com/45934191/129149789-b4ecd64a-ed0a-45f6-ba42-a32a46b83650.png)

  이 때 concat하면 채널이 점점 커지게 되고, (1배, 2배, 4배, 8배, ...) conv feature map의 채널 역시 같이 커지고 params 수도 같이 커진다.
  **따라서 중간에 channel을 한번씩 줄여줘야 한다!** ->`1*1` conv

  ![img](https://user-images.githubusercontent.com/45934191/129150114-1564981f-2e21-4afe-810a-89690bab633f.png)

  -> Dense Block으로 늘이고 다시 Transition Block으로 줄이는 것을 반복한다

  **간단한 neural network or 분류 문제에 있어서 DenseNet, ResNet 구조를 쓰면 웬만큼 성능이 잘 나온다**

  ## Summary

  Key takeaways : recepted field를 늘이는 입장에서,

  - VGG : `3*3` 반복.
  - GoogLeNet : `1*1` conv으로 채널 수를 줄여 params 줄임
  - ResNet : skip-connection으로 네트워크 깊게 쌓을수있게
  - DenseNet : concat

### (06강) Computer Vision Applications

- **Semantic Segmentation**

  - 이미지의 모든 픽셀의 라벨을 확인하고 싶은 것.

  - dense classification, per pixel라 불린다.

  - 활용 범위? 자율주행, ADAS, 운전보조장치

  - **Fully Convolutional Network**

    ![image-20220208133050841](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208133050841.png)

    - network 를 1 x 1 convolution layer로 만든다.

    - Colvolutionalization : CNN의 dense layer 을 없애는 과정.

    - CNN과 parameter 수가 같다. 

      ![image-20220208133730238](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208133730238.png)

      - \# of parameters
        - Left : (4 x 4 x 16) (flat) x 10 = 2,560
        - Rigth : 4 x 4 x 16 x 10 (kernel의 개수) = 2,560
      - This process is called **Convolutionalization**

    - why use convolutionalization?

      - Input 이미지의 크기 제한을 받지 않는다. input이 커지면 비례해서 output, special dimension도 커진다
        - How? convolution이 가지는 shared parameter 때문.
      - fully convolutional network로 단순히 분류가 아닌 feature map을 만들 수 있다.

    - output dimension의 크기가 줄어들게 되어, 원래의 사이즈로 돌려줘야함. 이를 upsampling이라고 한다.

    - upsampling된 feature map들을 종합해서 최종적인 segmentation map을 만든다. 각 픽셀당 확률이 가장 높은 클래스를 선정해주는 것.

      Ex) 만약 (1, 1) 픽셀에 해당하는 클래스당 확률값들이 강아지 0.45, 고양이 0.94, 나무 0.02, 컴퓨터 0.05, 호랑이 0.21 이런 식이라면, 0.94로 가장 높은 확률을 산출한 고양이 클래스를 (1, 1) 픽셀의 클래스로 예측

    - 다만, subsampling으로 정보가 줄어든 상황에서 이를 가지고 복원하는 셈이므로 성능이 그리 뛰어나지는 않다.

    - **Deconvolution (conv transpose)**

      ![image-20220208140813617](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208140813617.png)

      - convolution의 역연산 (엄밀하 말하면 역은 아님)

        Ex) 2 + 8 = 10, 3 + 7 = 10 결과만 보고 복원은 불가능함

- **Detection**

  - per pixel이 아닌 boundary 찾기

  - **R-CNN**

    ![image-20220208141258196](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208141258196.png)

    - R-CNN은 1) input image에서 2) 이미지가 있을 것 같은 2000개의 영역을 추출해 (using Selective search) 3) 똑같은 크기로 맞춘뒤 (using AlexNet3) 4) 분류한다 (usinv SVM)
    - 정확하지 않고, 오래걸림

  - **SPPNet**

    - R-CNN의 가장 큰 문제 : 2000개의 patch를 다 CNN을 돌려야 하나의 이미지를 확인 가능/ CPU에서 하나의 이미지당 1분 정도 소요됨...
    - SPPNet은 전체 이미지를 CNN **한 번만** 돌린 뒤, 뽑힌 bounding box위에 해당하는 convolution feature map의 tesor만!

  - **Fast R-CNN**

    - CNN을 한 번만 돌리지만 결국 해당하는 bounding box의 tesor를 다 분류해야하므로 느리다.
    - SPPNet의 concept를 그대로

    1) bounding boxes 
    2) CNN 한 번만 돌린다
    3) Feature Map 상에서 ROI pooling 사용해 Feature을 뽑아낸다.
       - ROI pooling? 참고 https://blog.naver.c`om/leesoo9297/221165682280

  - **Faster R-CNN**

    - Region Proposal Network + Fast R-CNN

    - selective search를 뉴럴 네트워크로 해결 즉, Region Proposal을 생성하는 방법 자체를 CNN 내부에 네트워크 구조로 넣어놓은 모델

    - Region Proposal Network

      - 이미지에서 특정 영역이 bounding box의 의미가 있는지를 판별. 즉 물체가 있는지 없는지 anchor boxes를 통해 판별

      - anchor boxes : 미리 정해놓은 bounding box의 크기. 어떤 크기의 물체가 있을 것 같다를 미리 정해놓음.![image-20220208143301141](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208143301141.png)

        - 9 : Three different region sizes (128,256,512) with three different ratios (1:1, 1:2, 2:1)

        - 4 : four bounding box regression parameters

        - 2 : box classification (whether to use it or not)

  - **YOLO**

    - Faster R-CNN의 bounding box를 따로 뽑아 따로 분류하는 과정없이 한 번에

    ![image-20220208143812236](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208143812236.png)

    - image를 SxS grid로 나눈다
      - 이미지 안에 찾고 싶은 물체 중앙이 해당 grid 안에 들어가면 그 grid set이 해당  물체의 bounding box와 물체가 무엇인지를 같이 예측해준다.

    ![제목 없음](C:\Users\82106\Desktop\박기련\공부\Ai-Tech\4주차\0208 (Day 14)\제목 없음.png)

    - 동시에 어떤 Class인지를 예측

    ![image-20220208144414614](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208144414614.png)

    - 취합
      - SxSx(B*5+C)
      - SxS: Number of cells of the grid
      - B*5:B bounding boxes with offsets (x,y,w,h) and confidence 
      - C:Number of classes

### (07강) Sequential Models - RNN

# Sequential Model

일상생활에서 접하는 대부분의 데이터는 seq data이다. (음성, 오디오, 비디오, 모션 등)

seq data를 처리하는 데 있어서 가장 큰 어려움은, 우리가 얻고싶은 것은 하나로 정의되는 label일 때가 많은데 seq 데이터는 길이가 언제 끝날지 모르기 때문에 내가 받아야 하는 입력의 차원을 알기 힘들다. fully connected layer, conv neural network 사용할 수가 없다.

**따라서 몇 개의 입력이 들어오든 동작하는 모델을 구현해야 한다.**

### Naive sequence model

![img](https://user-images.githubusercontent.com/45934191/129159875-a4ed89f0-abdb-45f3-8237-84a11cad8473.png)

- 가장 기본적인 seq 모델
- 이전 데이터가 들어왔을 때, 다음 번 데이터 예측을 하는 것 (ex. language model)

t개의 data가 있을 때, 두번째는 첫 데이터만, 세번째는 1,2 데이터만 .. 이런 식으로 내가 고려해야 하는 vector 수가 점점 늘어난다. **내가 고려해야 하는 정보의 양이 점점 늘어나는 것이 특징이다.**

#### Autoregressive model

![image-20220208145004429](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220208145004429.png)

- 과거 데이터 일정 갯수만큼만 고려하는 seq model

#### Markov model (first-order autoregressive model)

![img](https://user-images.githubusercontent.com/45934191/129160567-aa57da84-1b1c-4d10-b8af-da513c9e3f65.png)

- 나의 현재는 과거에만 dependent하다고 가정한다. 이 때 과거는 바로 전의 과거이다.
  ex) 내일의 수능 점수는 전날 공부한 것에만 dependent하다
- 이 모델은 많은 정보를 버릴 수밖에 없게 된다.
- joint distribution 표현이 굉장히 쉬워지는 반면 (9강 참조), 과거의 정보를 많이 고려할 수 없고 많은 정보를 버리게 된다.

#### Latent autoregressive model

![img](https://user-images.githubusercontent.com/45934191/129160630-9df449b7-41eb-4ce7-be36-804d3451d54e.png)

- 중간에 hidden state를 하나 넣고, hidden state가 과거의 정보를 summarize하고 있다고 가정한다.
- 다음번 time step은 hidden state 하나에만 dependent하다
- 하나의 과거 정보에만 dependent하는 것 같지만, 이전의 정보를 요약하는 hidden state (laten state)를 고려하는 것
- hidden state를 어떻게 만드냐에 따라 모델의 성능에 차이가 있다

## Recurrent Neural Network (RNN)

![img](https://user-images.githubusercontent.com/45934191/129161446-b6ad7267-6637-40fa-b0c6-94e4cc9bcf8d.png)

- 이런 개념들을 가장 쉽게 설명할 수 있고, 쉽게 구현한 방법
- MLP와 비슷하지만, 자기 자신으로 돌아오는 구조가 있다는 것이 차이점이다
- *ht*(hidden state)는 *xt* 뿐 아니라 이전의 *xt-1*에도 dependent하다.

![img](https://user-images.githubusercontent.com/45934191/129161548-57477d11-cf16-472f-a3c0-a14a1ab146b1.png)

- RNN : 시간 순으로 문제를 푼다
- 1 RNN = input : 현재의 입력 + 이전 입력
  time step t에서 보는 것은, t-1에서 전달된 정보이다. (시간 순)
- *x*0 : A 모델 -> return된 정보+*x*1 = *h*1 -> 그 정보+*x*2 = *h*2 -> ...

!! 중요 RNN과 같이 recurrent 구조가 있는 것을 시간 순으로 풀게 되면, 입력이 굉장히 많은 fully connected layer로 표현될 수 있고, 이렇게 푸는 것이 RNN 학습방법이다.

RNN 자체로는 복잡하지만 만약 time을 fix하고, 시간 순으로 푼다면 각각의 네트워크가 parameter을 공유하는, input이 매우 큰 하나의 네트워크가 된다.

### Short-term dependencies / Long-term dependencies

![img](https://user-images.githubusercontent.com/45934191/129161612-99ba2fb4-5051-458a-8894-17337b27cca2.png)

![img](https://user-images.githubusercontent.com/45934191/129161657-4be862d9-3dc7-4651-b725-1a1fb1bb4932.png)

- RNN의 가장 큰 단점
- 과거에 얻어진 정보들이 다 취합되어서(summary) 미래에서 고려되어야 하는데, RNN은 이 정보들을 계속 취합하기 때문에 이전의 먼 과거의 정보가 미래까지 살아남기 힘들다.
- 몇 step 전의 short term dependencies는 현재에도 고려가 잘 되지만, 먼 과거의 정보는(long-term dependencies) 고려하기 힘들어 진다.
- 예) 음성인식, 문장이 나오면 답을 내는 문제 : 문장이 길어져도 이전 중요한 정보를 가지고 있다가, 계속 요약해서 필요할 때 써야 하는데, 5초 전의 대화를 기억을 하나도 못한다면, 이 model이 할 수 있는 사고는 매우 제한적이게 된다.

## RNN 학습은 왜 어려울까?

![img](https://user-images.githubusercontent.com/45934191/129161686-f3dd26c4-3d48-4ae2-a67e-a2c5cf5390a6.png)

- 네트워크를 풀게되면, 굉장히 큰 네트워크(width 가 매우 큰)가 된다.

- W : 가중치, *ϕ* : non-linear activation function
  *h*1​ = *h*0​에서의 입력값 + 현재 *x*1​값
  *h*2​ = *h*1​에서의 입력값 + 현재 *x*2​값
  = (*h*0​+*x*1​) + *x*2​
  중첩되는 구조가 들어가 있는 것을 확인할 수 있다.

  따라서 *h*0가 *h*4까지 가기 위해서는 굉장히 많은 (같은 W를 곱하고, activation function 통과) 연산을 거쳐야 한다.

- **Vanishing gradient** : 만약 (1) *ϕ*가 sigmoid 함수라면, (0<=sigmoid<=1. 정보를 줄인다)
  *h*0​에서의 정보를 계속 연산시 의미 없는 값이 되어 버린다. (값이 줄어든다)

  **Exploding gradient** : (2) activation function이 ReLu이고 W>0이라면, W라는 숫자를 n번 곱하게 되므로 *h*0 값에 매우 크게 dependent 하게 되고, 학습시 네트워크가 터져 학습이 되지 않는 현상이 발생한다. (그래서 RNN에서는 ReLu 사용을 지양)

## Long Short Term Memory (LSTM)

위의 단점을 해결해보자!

### vanilla RNN 구조

![img](https://user-images.githubusercontent.com/45934191/129164861-8b4f1b05-55df-4203-a7a1-4648e64729c8.png)

- x -> 네트워크 통과(W) + 이전 데이터 concat -> 네트워크 통과(W) , activation function 통과 -> .. -> output이 다음 번 네트워크의 input

### LSTM 구조

![img](https://user-images.githubusercontent.com/45934191/129164927-4aae0d55-2ee3-4f5e-9fed-13b4ed120e84.png)

- 훨씬 복잡해졌다!
- **각각의 component의 동작 방법, 왜 long term dependency를 잡는 데 도움이 되는지 알아보자**

![img](https://user-images.githubusercontent.com/45934191/129164959-5b68b400-4b27-4d6d-bef5-334deca2d369.png)

- *xt* : t번째 time step의 입력
- previous cell state
  - 네트워크 내부에서 0~t까지의 t+1개의 정보를 summarize
- previous hidden state (previous hidden output)
  - output이 위/아래로 흘러가면 -> t+1번째 lstm의 previous hidden state로 들어간다
- lstm의 입력
  - 이전의 출력값
  - 이전의 previous hidden state
  - previous cell state
  - 입력 3, 출력3 (실제로 나가는 것은 hidden state 하나)

### Core idea

![img](https://user-images.githubusercontent.com/45934191/129165041-e4a0888e-1508-4860-9aa2-3f119ff9ecbf.png)

cell state : time step t까지의 정보를 요약한 것

gate : 컨베이너 벨트라고 보면, 매번 t마다 벨트에 물건이 올라올 때, 어떤 물품을 올리고 어떤 물품을 내릴지에 대한 정보

### Gate

LSTM을 이해할 때, gate 위주로 하면 좋다.

![img](https://user-images.githubusercontent.com/45934191/129165093-e8496f0c-ceb4-483b-9d8d-47680aa5d16e.png)

(1) Forget gate

- 어떤 정보를 버릴지. 입력으로 들어가는 것은 현재의 입력 *xt*, 이전의 출력 *ht−1* -> 시그모이드를 통과한 후, *ft* 출력 (0<=*ft*<=1)
- *ft* : previous hidden state와 현재의 입력으로, 이전의 cell state에서 넘어온 정보 중 취할 것/버릴 것 결정한다.

(2) Input gate

- 현재 입력값을 검증 후 cell state box에 올린다
- *it* : 어떤 정보를 cell state에 추가할지 결정.
- *Ct*(cell state candidate) : 모든 값이 -1~1로 정규화 된 값. *Ct*가 현재 정보, 이전 출력값으로 만들어 진 일종의 cell state 예비군
- **이전 cell state와 , 현재 정보, cell state candidate로 update하게 된다.**

![img](https://user-images.githubusercontent.com/45934191/129165125-5b436ebd-2f06-47a1-9c24-ddc27c1395bc.png)

Update cell

- Forget gate에서 나온 *ft*값에, 이전 *Ct−1*을 곱하고 (버릴 것 결정) + input gate에서 나온 *Ct*와 *it*로 어느 값을 올릴 지 결정한다

(3) Output gate

- 이전의 결과물을 그대로 출력할 수도 있다 (GIU)
- LSTM : 어떤 값을 밖으로 출력할지 결정하는 GATE. gate만큼 연산해서 다음 hidden state로 흘러가게 된다

![img](https://user-images.githubusercontent.com/45934191/129165203-eb229696-41a5-47ef-9c8c-6d4c868bd5e6.png)

- \`previous cell state\`, \`previous hidden state\`, `Xt`가 neural network 안으로 들어온다

- 이전 cell state 중 지울 것 정하고 (\`Forget gate\`)

  \`previous hidden state\`, `Xt `로 어떤 값을 올릴지, `Ct`를 정한다

- 그 후 update 된 `cell state`, `candidate state`로 새로운 `cell state` 만들고,

- 그 정보 중 밖으로 보낼 정보를 선택한다.

### Gated Recurrent Unit(GRU)

![img](https://user-images.githubusercontent.com/45934191/129165285-0383d9ef-f826-46cc-afae-e2d5d6dd88db.png)

- reset gate, update gate 2개 (recet gate = forget gate와 비슷)
- 앞의 lstm과의 차이점은 , hidden state가 곧 output이라는 점 (cell state X) -> output gate가 필요없다
- 같은 task에 대해, lstm을 활용할 때보다 GRU를 활용할 때 params 가 더 적고, 성능이 올라가는 경우를 꽤 많이 본다.
- 적은 params로 동일한 output을 내면 generalization performance가 좋아지기 때문
- 둘 다 요즘은 많이 활용되지 않는다 -> transformer 구조

# 2. 퀴즈 리뷰

- [퀴즈] CNN

# 3. 과제 리뷰

- [기본 과제\] CNN Assignment
- [기본 과제\] LSTM Assignment

# 4. 피어세션 정리

- 강의 리뷰 및 QnA
  - CNN
  - (07강) Sequential Models - RNN

# 5. 회고

CNN 복습 철저히. RNN 준비할 것!
