# 1. 2월 9일 (Day 15) 강의 정리

### (08강) Sequential Models - Transformer

- Sequential Model

  ![image-20220209221425052](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220209221425052.png)

- **Transformer**

### (09강) Generative Models 1

### (10강) Generative Models 2

# 2. 퀴즈 리뷰

- [퀴즈] RNN
- [퀴즈] Generative Model

# 3. 과제 리뷰

- [기본 과제\] Multi-head Attention Assignment

# 4. 멘토링

- 시각화 알아두자

- 자기만의 코트 templet을 만들어보자. 깃허브로 돌아다니며

- 사전 질문

  - Mnist data의 경우 들어오는 data의 pixel이 정해져 있기 때문에 따로 transform을 시켜줄 필요가 없는데, 그렇다면 우리가 직접 만든 data의 input의 pixel이 전부 다르다면, 이를 각각 transform시켜주는 pre-processing 단계를 거쳐야 하나요? 그렇다면 이 pre-processing 과정에서 원본 데이터의 정보가 손실되지는 않나요?

    흔히 Parameter를 줄이는 방식(ex AlexNet -> VGGnet -> GoogleNet)으로 전에 있는 모델을 발전시켜 나가는 것 같은데(overfitting 방지나 generalization performacne를 높이기 위해), GPT-2, GPT-3와 같은 모델은 parameter가 1750억개라고 합니다. 파라미터를 늘리는 것은 어떤 효과를 불러올 수 있나요? 

    -> 학습이 많으면 역량은 높아짐. 대신 적어지면 capacity이 좋다. 즉, 무턱대고 많다면 overfitting 등이 발생하므로, 효율적으로 쌓아야하고, 그런 모델을 찾는 것이 목표이다.

  - 1x1 convolution과 같이 channel의 depth를 줄여 parameter의 수를 줄이는 과정을 진행하게 되는데, 이때 channel의 depth를 줄이면 어떤 변화가 일어나나요?

    -> 기본적으로 연산량이 줄어든다. 검색해볼 것

- Kaggle을 활용한 인공지능 공부 (for P stage)

# 5. 피어세션 정리

- 강의 리뷰 및 QnA

# 6. 회고
