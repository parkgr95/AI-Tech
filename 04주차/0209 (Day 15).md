# 1. 2월 9일 (Day 15) 강의 정리

### (08강) Sequential Models - Transformer

- **Sequential Model**

  ![image-20220209221425052](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220209221425052.png)

  - 이러한 특성 때문에 sequential model은 다루기 어렵다

- **Transformer**

  - **개괄적인 수준의 설명**

    ![image-20220210101100704](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210101100704.png)

    - 재귀적 구조 x attention이라는 구조를 활용한 sequence model

    ![img](https://nlpinkorean.github.io/images/transformer/The_transformer_encoders_decoders.png)

    - nlp가 아닌 cv 등 다양한 분야에 사용 가능하다

    ![image-20220210102643639](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210102643639.png)

    - 불어에서 영어로 문장을 번역하고 있다 (sequence to sequence model).

      ->  '입력 sequence의 domain (불어)과 출력 sequence의 domain (영어)는 다를 수 있다'를 알 수 있다.

    - 입력의 단어의 개수 (3개), 출력의 단어의 개수(4개)가 다르다.

      ->  '입력 sequence와 출력 sequence의 단어의 수는 다를 수 있다'를 알 수 있다.

    - model이 1개다.

      -> 입력 개수에 상관없이 한 번에 encoding 할 수 있다. 단, generation 할 때는 (한 단어씩) autoregressive 한다.

    - 이해해야할 것! 1) 어떻게 n 개의 단어가 encoder에서 한 번에 처리가 되는가 2) encoder와 decoder 사이에 어떤 정보를 주고받는가 3) decoder가 어떻게 generization 할 수 있는가

    ![image-20220210102906550](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210102906550.png)

    - 1\) 어떻게 n 개의 단어가 encoder에서 한 번에 처리가 되는가?
      1. n 개의 단어 (vector)가 첫번째 Encoder에 들어감.
      2. Encoder는 Self-Attention와 Feed Forward Neural Network로 이루어진 구조
      3. Encoder를 거친 n 개의 단어가 두번째 Encoder에 들어감
      4. 반복
    - Self-Attention이 Transformer가 왜 잘되는지를 보여줌. FFNN은 MLP 구조와 비슷함

  - **벡터/텐서들을 기준으로 모델 살펴보기**

    ![img](https://nlpinkorean.github.io/images/transformer/embeddings.png)

    - 단어가 들어오면 기계가 번역할  수 있게 각 단어마다 특정 숫자의 vector로 표현

    ![image-20220210104809310](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210104809310.png)

    - ! **Self-Attention은 하나의 vector x1이 z1으로 넘어갈때, x1의 정보만 활용하는 것이 아니라 x2, x3의 정보를 활용한다**
    - 즉, n 개의 단어가 주어지면 각각의 i번째 벡터를 바꿀때 나머지 n-1개의 벡터를 고려한다. dependencies가 있다.
    - Feed Foward는 dependencies가 없이 그냥 통과

    ![img](https://nlpinkorean.github.io/images/transformer/encoder_with_tensors_2.png)

    - 두 개의 단어를 생각해보자.

      ![image-20220210105458793](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210105458793.png)

      - it이 어떤 단어에 dependent 한 지 알아야한다. 즉, 단어가 그 문장속의 단어들과 어떤 interaction이 있는 지를 이해하는 게 중요

      - Transformer의 역할 : 단어들과의 관계성을 파악해, 그 단어들을 더 잘 표현한다.

    ![image-20220210105926508](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210105926508.png)

    - Attention은 각 단어마다 3가지 vector (neural network)를 만들게 된다. Query, Key, Value

    ![image-20220210110135984](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210110135984.png)

    - 3 vector를 사용해 단어를 encoding 해준다.
    - 'Thinking'을 encoding 해준다고 생각할때

    ![image-20220210110348653](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210110348653.png)

    - Thinking이라는 x1 단어의 Score vector를 만들게 된다.

    - Score을 구할 때 x1의 Query와 나머지 vector의 Key를 구한 뒤 내적한다. 즉, 이 단어가 나머지 단어와 얼마나 관계가 있는 지, 유사도를 보고 얼마나 interaction 할 지 정하게 된다. 이것이 attention이다.

    - attention : Task를 수행할 때 어떤 특정 time step에 어떤 입력을 주의깊게 볼지 결정하는 기법

    ![image-20220210111136428](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210111136428.png)

    - 구한 score를 normalize 해준다. 너무 커지지 않게 해준다.
    - key vetcor 또는 query vector의 dimension에 dependent하여, 현재 dimension인 64의 루트를 취해준 8로 score를 나눠준다. (q와 k의 차원은 같다)
    - softmax를 취해준다.

    ![image-20220210111750817](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210111750817.png)

    - 각각의 Value vector에 이 값을 곱한다.

    - 이 weighted value vector들을 다 합하면, 이것이 현재 단어에 대한 self-attention layer의 출력이 된다. 

    - 주의할 점! 

      -> Q 벡터와 K 벡터의 차원은 같아야한다. 내적을 해야하기 때문. V 벡터는 달라도 괜찮다. Encoding 된 벡터와 V 벡터의 차원은 같다. (MHA에선 다름)

    ![image-20220210113612511](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210113612511.png)

    - X의 의미 : 2개의 단어, 각 단어마다 4차원으로 표현

    ![image-20220210113742901](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210113742901.png)

    - 파이토치에선 한두줄
    - 왜 잘되는 걸까? Transformer는 input, network가 고정되어 있어도, encoding 하려는 단어와 옆의 단어들의 값에 따라, encoding 된 값이 달라지게 된다. flexible 한 모델. 많은 걸 표현할 수 있는 모델
    - n 개의 단어를 한번에 처리해야하고, computational task가 n^2에 비례하기 때문에 length가 길어지면 처리할 수 있는 한계가 있다.

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_attention_heads_qkv.png)

    - multi-headed attention : 앞의 attention을 여러번 수행. Q, K를 여러개 만든다.

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_attention_heads_z.png)

    - n 개의 attention을 반복해 n 개의 encoding된 vector를 얻을 수 있다.
    - 고려해야할 점 : 입력(embedding된 vector)과 출력(encoding된 vector)의 차원을 맞춰줘야함

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_attention_heads_weight_matrix_o.png)

    - 차원을 다시 줄여준다. (linear map)

      Ex) 80 (원래는 10인 vector가 8개) 차원인 encoding 된 vector 에 (80 x 10)인 행렬을 곱해줌

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_multi-headed_self-attention-recap.png)

    - 실제 구현에서는 다름. (실습 참고)
      - 논문 : k 개의 head를 만들고, aggregate
      - 구현 : 하나의 dimension을 k 개로 나눔

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_positional_encoding_vectors.png)

    - **positional encoding** : 입력에 특정 값 (pre-defined)을 더해줌. bias
    - 왜 필요한가? encoding은 순서에 independent함. 그렇기에 문장을 만들 때 단어의 순서는 중요하지 않음. 그렇기에 주어진 encoding에 값을 더한다.

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_positional_encoding_example.png)

    - 4차원 encoding일때

    ![image-20220210115701650](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210115701650.png)

    - 각 단어에 벡터에 해당하는 값을 더해줌. 일종의 offset을 준다

    ![image-20220210115945587](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210115945587.png)

    - 각 encoder 내의 sub-layer 가 residual connection으로 연결되어 있으며, 그 후에는 layer-normalization과정을 거친다.

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_resideual_layer_norm_3.png)

    - Decorder : encorder로 나온 정보를 가지고 출력.
    - 2\) encoder와 decoder 사이에 어떤 정보를 주고받는가 

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_decoding_1.gif)

    - input에 있는 단어들을 decorder에 있는 출력하고자 하는 단어들에 대해서 attention map을 만드려면, inpjut의 단어들의 Key (K), Value (V) 벡터가 필요하다.

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_decoding_2.gif)

    - decoder에서 만들어진 Query (Q) 벡터와 입력의 K, V 벡터를 가지고 최종적으로 출력 (autoregressive 하게, 하나씩)

    ![image-20220210131759300](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210131759300.png)

    - Masking : 학습을 위해 이전 단어들만 dependent하게 만드는 방법. 미래의 정보를 사용하지 않음

    ![image-20220210131949775](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210131949775.png)

    - Encoder-Decoder Attemtion : Encoder, Decoder 사이의 관계
    - Query는 이전 encoder로 주어진 정보로 만들고, Key, Value는 encoder의 정보를 활용하겠다.

    ![img](https://nlpinkorean.github.io/images/transformer/transformer_decoder_output_softmax.png)

    - decoder에서 나온 출력은 Linear layer 와 softmax layer를 통과하여 최종 출력 단어로 변환된다.

- **Vision Transformer**

  ![image-20220210132346730](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210132346730.png)

  - 이전 transformer은 번역에서 활용됐음
  - 이미지에서 많이 활용됨

- **DALL-E**

  ![image-20220210132444005](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220210132444005.png)

  - decorder만 활용해 문장에 해당하는 이미지를 생성했다.

### (09강) Generative Models 1

- **Learning a Generative Model**

  - Generative Model은 단순히 이미지나 문장을 생성하는 것이 전부가 아니다. 우리가 알고 있는 부분보다 더 많은 부분을 차지하고 있는 것이 Generative Model이다. 만약에 강아지 이미지 데이터셋이 주어졌다고 가정해보면, 이 데이터셋을 가지고 우리는 무엇을 할 수 있을까? 
    - **Generation** : 데이터셋에 없는 (**train**되어 있지 않은) 강아지 이미지를 새로 생성해 줄 수 있다. (**sampling**)
    - **Density estimation** : 어떤 이미지가 모델에 들어왔을 때, 강아지인지 아닌지 구분하는 이상탐지 (**anomaly detection**)도 할 수 있다. (= **Explicit model**)
      - **Explicit model** : 어떤 입력이 주어졌을 때, 그것에 대한 확률값을 얻어낼 수 있는 모델. 
      - **Implicit model** : 단순히 Generation을 하는 모델. 
    - 즉, Generative Models이 fully 학습됐다는 건, 만들어낼 수 있을 뿐 아니라 구분해낼 수도 있다는 것이다.
    - **Unsupervised representation learning** : 또한 생성모델을 활용해서 **feature learning**을 수행할 수 있다.
    - 여기서 나오는 p(x)를 만들기 위해서는 기본적인 분포들을 이해해야 한다.

- **Basic Discrete Distributions**

  - **Bernoulli distribution** : 이항분포로 표현하는 변수는 p하나가 존재한다. p를 알면 나머지는 1-p가 되니까. e.g. 동전의 앞 뒤
  - **Categorical distribution** :  필요한 변수의 개수는? 만약 n개의 경우가 존재한다고 했을 때, n-1개의 변수이다. 모든 확률의 합은 1이 되니까 n-1개만 알아도 나머지 한개는 자연스럽게 알 수 있으므로. e.g. 주사위
  - 하지만, 변수가 많아지면 많아질수록 학습은 더욱 어려워진다. 파라미터의 수를 줄일 수 있는 방법은? n개의 사건들이 독립적이라는 가정을 해보는 것.
    - 물론, 이 가정은 말이 안되는 가정이다. e.g. 인접한 픽셀의 색이 모두 다르다?
    - 이 경우에서 얻어지는 possible states? 2^n (선택지가 2가지일 경우).
    - 이 때 필요한 파라미터의 수? n (각각의 픽셀에 1개씩만 있으면 되니까)
    - 이렇게 나올 수 있는 이유? 각 변수들이 서로 독립이기 때문에

- **Conditional Independence**

  - 그래서 처음에 얻었던 2^n개 (Fully dependent)와 n개 (Fully independent) 사이의 parameter를 구하는 모델을 위해, 필요한 rule 3가지가 있다.

    - **Chain rule** : n개의 joint distribution을 n개의 conditional distribution으로 바꿔주는 방법. chain rule은 각 변수끼리의 독립여부와 상관없이 항상 만족한다.
      $$
      p(x_1,...,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,...,x_{n−1})
      $$

    - 

    - **Bayes' rule**
      $$
      p(x|y)={p(x,y) \over p(y)}={p(y|x)p(x) \over p(y)}
      $$

    - **Conditional independence** : x, y, z가 주어졌을 때, x와 y가 독립이라면, 아래의 수식을 만족.
      $$
      p(x|y,z)=p(x|z)
      $$

  - 이 중 **Chain rule**과 **Conditional independence**를 사용해 좋은 모델을 만들 것이다. (**Auto-regressive**)

  - **Chain rule**만 사용한다면, parameter의 개수는 2^-1일 것이다. 

    - Why? 어떠한 가정도 하지 않았기에, fully denpent한 모델과 같은 parameter 수를 가졌다.

  - **Markov assumption**을 가정해보면, joint distribution은 아래와 같이 표현할 수 있다.
    $$
    p(x_1,...,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_2)...p(x_n|x_{n−1})
    $$

  - 이렇게 가정하면 parameter는 2n−1개가 된다. 이런 conditional independency을 활용한 모델을 **Auto-regressive Model**이라고 한다.

    - **Markov assumption** : 과거와 현재 상태가 주어졌을 때의 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정된다는 것을 뜻한다 (From Wiki) (i+1번째의 사건은 i번째 사건에만 종속)

- **Auto-regressive Model**

  - auto-regressive model :  현재 정보는 이전 정보에만 dependent하다. 즉, i 번째가 i-1 번째에 depent한 것뿐만 아니라 i 번째가 i-1 ~ 1 번째 (모든 history)에 dependent한 것도 auto-regressive model이다.
  - 바로 이전의 정보만 고려한 모델을 ar 1 model, 이전 n 개의 정보를 다 고려한 모델을 ar n model이라고 한다.
  - 이미지들 마다 임의로 순서매겨야한다. 하지만, 정해진 순서 매기는 방법이 없기에, 순서, 방법에 따라 성능, 방법론도 달라질 수 있다.

- **NADE: Neural Autoregressive Density Estimator**

  - **NADE**는 i번째 값이 이전의 모든 값과 dependent한 ar n model이다. 따라서 neural network weight가 점차 늘어난다.  

  - **NADE** 는 explicit model이다. 즉, 주어진 input에 대해 확률을 계산할 수 있다.

  - 이름에 density가 들어가면 explicit model일 확률이 높다
  - 만약에 continuous random variables라면, **Gaussian mixture model**을 활용해서 continuous distribution을 만들 수 있다.
    - **Gaussian mixture model** : 가우시안분포를 선형결합하여 만들어진 분포

- **Pixel RNN**

  - pixel을 생성하기 위한 auto-regressive 모델
  - RNN을 통해서 Generate
  - 앞에 모델들은 fully connected network임에 반해, RNN을 통해 generate
    - Row LSTM : 해당 픽셀의 위쪽에 있는 픽셀들을 활용하고
    - Diagonal BiLSTM : i번째 이전의 정보를 활용하는 차이가 있습니다.

### (10강) Generative Models 2

- **Latent Varialbe Models**

  - autoencoder은 generative model인가? 아니다. 핵심은 variational autoencoder가 generative model이 되는 이유가 뭔지 아는 것!

- **Variational Auto-encoder**

  - **Variational inference (VI)**의 목적 : 관심 있는 posterior distribution을 잘 근사할 수 있는 variational distribution을 찾는 것!

    - **Posterior distribution : p_θ(z|x)** : 나의 관찰값이 주어졌을 때, 내가 관심있는 random variable에 대한 확률분포. 지금처럼 x와 z 뒤바뀐 걸 likelihood라고 불린다.
      - z : latent vector
    - **Variational distribution : q_ϕ(z|x)** : 일반적으로 posterior distribution을 구하기 어려울 때가 많이 있어서, 이 분포를 학습할 수 있는(최적화 시킬 수 있는) 분포로 근사하는 분포.

  - 이 찾는 과정을 **VI**라 부른다. (aka Encoder)

  - 최적화하기 위해선 loss function이 필요하다! VI에서는 KL divergence라는 metric을 활용해서 variational distribution과 posterior의 차이를 줄인다.

    ![image-20220213223656456](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220213223656456.png)

  - 문제는 posterior가 뭔지 모르는 데 variational을 찾는다는 것. 이것을 가능하게 하는 것이 바로 **ELBO** 트릭이다.
    $$
    lnp_θ(D)=\mathbb{E_{q_ϕ(z|x)}}[ln\frac{p_θ(x,z)}{q_ϕ(z|x)}]+D_{KL}(q_ϕ(z|x)||p_θ(z|x))
    $$

  - 뒤의 항인 KL diverge를 줄이는 것이 목적이지만 불가능하기 때문에, 앞의 항인 ELBO를 최대화하여 반대급부로 원하는 object를 얻는 방식을 활용한다.

  - 정리하자면, 우리의 목적은 입력 x에 대해 latent space z를 찾는 것이다. 계산할 수 없는 latent space의 확률 분포 posterior과 최적화하려는 variational 사이의 거리 (kl diverge로 정의된, psudo metric으로 정의된 거리)를 줄이려는 목적을 ELBO의 maximize함으로써 얻어낸다.

  - **ELBO (Evidence Lower Bound)**는 아래와 같이 전개 된다.
    $$
    \mathbb{E_{q_ϕ(z|x)}}[ln\frac{p_θ(x,z)}{q_ϕ(z|x)}] = \int ln\frac{p_θ(x|z)p(z)}{q_ϕ(z|x)}q_ϕ(z|x)dz = \mathbb{E_{q_ϕ(z|x)}}[P_θ(x,z)] - D_{KL}(q_ϕ(z|x)||p_θ(z|x))
    $$
    

    - Reconstruction Term : 마지막 수식의 첫 번째 항. encoder를 통해 입력을 latent space로 보냈다가 decoder로 돌아오는 auto-encoder의 reconstruction loss을 줄인다.

    - Prior Fitting Term : 두 번째 항. latent space에 올라간 입력들의 점이 이루는 latent distribution이 prior distribution과 비슷하도록 해주는 Term이다.

  - Variational Auto-Encoder가 generative model이 되는 이유이다. 엄밀히 말하면 explicit한 모델이 아닌 implicit 모델이다.

  - 정리하자면, 입력을 encoder로 latent space로 보내서, 무언가를 찾고, reconstruction하는 term으로 만들어지는데, prior fitting term으로 z를 sampling 하고, decoder를 타고 나온 output을 generation result로 보기 때문에 gerative model이다. autoencoder 단지 input이 latent space로 갔다가 output으로 나오기 때문에 generative model로 보기 어렵다.

  - Key limitation:

    - VA는 explicit이 아니기 때문에 가능도를 추정하기 어렵다. (intractable mode)

    - prior fitting term은 최적화를 시키기 때문에 미분가능해야 한다. KL은 적분이 들어가 있어서 계산하기 힘들 수 있다. 그래서 KL 과 비슷한 Gaussian을 사용한다.

    - 모든 output 차원이 서로 독립인 isotropic Gaussian을 사용한다.
      $$
      D_{KL}(q_ϕ(z|x)||\mathcal{N}(0,1))=\frac{1}{2}\sum_{i=1}^{D}(σ_{z_i}^2+μ_{z_i}^2−ln(σ_{z_i}^2)−1)
      $$

- **AAE (Adversarial Auto-encoder)**

  ![image-20220213234246252](C:\Users\82106\AppData\Roaming\Typora\typora-user-images\image-20220213234246252.png)

  - 앞에서 언급한 VAE의 가장 큰 단점은 Gaussian이 아닌 경우에는 활용하기가 어렵다에 있다. 
  - AAE는 **GAN**을 활용해서 latent distributions의 분포를 맞춰준다. VAE의 prior fitting term을 GAN으로 바꿔준 것이다.
  - sampling만 가능한 분포만 있으면 맞춰줄 수 있다. e.g. uniform distribution, gausian mixed model.

- **GAN (GenerativeAdversarialNetwork)**

  - enerator는 Discriminator를 속이기 위해 학습하고, Discriminator는 Generator가 생성한 것을 판별하기 위해 계속 발전하는 방법론이다. 

  - 즉, discriminator 가짜와 진짜를 구분하는 분류기를 학습하고, gerator는 그렇게 학습된 discriminator 입장에서 true가 나오도록 generator를 업데이트하고, 이를 반복한다.

  - 가장 큰 장점 : 학습의 결과를 내주는 generator를 학습하는 discriminator가 점차 좋아지는 것.

  - implicit model이다.

  - 학습 원리를 살펴보면, 먼저 discriminator 입장에서 다음과 같은 식을 얻을 수 있다.
    $$
    \max_D V(G,D) = E_{x∼p_{data}}[logD(x)]+E_{x∼p_G}[log(1−D(x))]
    $$

    - 이 식을 최적화 (최대화) 하는 D (optimal discriminator)는 다음과 같다.
      $$
      D_G^*(x) = {p_{data}(x) \over p_{data}(x) + p_G(x)}
      $$

    - optimal discriminator : gernerator가 fix되어 있을 때, discriminator 를 최적으로 갈라준다.

  - 이것을 다시 generator 입장에서
    $$
    \min_D V(G,D) = E_{x∼p_{data}}[logD(x)]+E_{x∼p_G}[log(1−D(x))]
    $$

  - optimal discriminator를 generator에 넣게 되면 다음과 같은 식을 얻을 수 있다.
    $$
    \begin{align} 
    V(G,D_G^*(x)) & = D_{KL}[p_{data}, {p_{data} + p_G \over 2}] + D_{KL}[p_G, {p_{data} + p_G \over 2}] − log4 \\
    & = 2D_{JSD} [P_{data}, P_G] - log4
    \end{align}
    $$

  - 여기서 앞의 식을 Jenson-Shannon Divergence (JSD)라고 부르며, 이것을 최소화하는 것이 목적이 될 수 있습니다.

  - 하지만, 실질적으로 Discriminator가 optimal discriminator에 수렴한다고는 보장할 수 없고, 그렇게 됐을 때 Generator가 학습이 잘될 수 있다는 보장이 없어서 애매한 부분이 있습니다.

- GAN 논문들

  - DCGAN : 이미지 도메인에서 GAN을 활용했으며 reakyrelu를 사용한다.
  - Info-GAN : 학습을 할 때 단순히 G로 이미지를 만드는게 아니라 class 라는 원핫벡터 C를 매번 랜덤하게 집어넣어 생성할 때 GAN이 특정 모드 (C)에 집중할 수 있게 해준다.
  - Text2Image : 문장이 주어지면 이미지를 만든다.
  - Puzzle-GAN : 이미지 안의 작은 부분 (sub-patch)들을 가지고 이미지를 복원한다.
  - CycleGAN : GAN구조를 활용하지만 이미지간의 도메인을 바꿀 수 있으며 Cycle-consistency loss를 활용한다.
    - Cycle-consistency loss <- 굉장히 중요하다. 꼭 알아둘 것.
  - Star-GAN : style transfer와 유사하며 이미지를 단순히 다른 도메인으로 바꾸는게 아니라 컨트롤할 수 있게 한다.
  - Progressive-GAN : 고차원의 이미지를 잘 만들 수 있으며 낮은 해상도(차원)부터 높은 해상도(차원)까지 픽셀을 키우는 progress training의 아이디어를 활용했다.
  - 이렇게 GAN을 활용한 논문은 아주 많이 나오고 있으니 전부 보는것은 힘들지만 중요한 아이디어는 이해하는 것이 좋다.

# 2. 퀴즈 리뷰

- [퀴즈] RNN
  1. Forget gate, input gate, output gate의 총 3개의 gate를 가지며, 모든 gate와의 연산을 진행한 후 cell state를 업데이트 합니다LSTM의 gating unit에 대한 설명으로 가장 부적절한 것은?
  2. "I love you"라는 문장이 주어졌을 때, self-Attention을 통과한 후의 token 'I'의 representation을 구하세요.
- [퀴즈] Generative Model
  1. VAE는 AE와 다르게, encoder와 decoder로 이뤄져 있습니다
  2. GAN에 대한 설명 중

# 3. 과제 리뷰

- [기본 과제\] Multi-head Attention Assignment

# 4. 멘토링

- 시각화 알아두자

- 자기만의 코트 templet을 만들어보자. 깃허브로 돌아다니며

- 사전 질문

  - Mnist data의 경우 들어오는 data의 pixel이 정해져 있기 때문에 따로 transform을 시켜줄 필요가 없는데, 그렇다면 우리가 직접 만든 data의 input의 pixel이 전부 다르다면, 이를 각각 transform시켜주는 pre-processing 단계를 거쳐야 하나요? 그렇다면 이 pre-processing 과정에서 원본 데이터의 정보가 손실되지는 않나요?

    흔히 Parameter를 줄이는 방식(ex AlexNet -> VGGnet -> GoogleNet)으로 전에 있는 모델을 발전시켜 나가는 것 같은데(overfitting 방지나 generalization performacne를 높이기 위해), GPT-2, GPT-3와 같은 모델은 parameter가 1750억개라고 합니다. 파라미터를 늘리는 것은 어떤 효과를 불러올 수 있나요? 

    -> 학습이 많으면 역량은 높아짐. 대신 적어지면 capacity이 좋다. 즉, 무턱대고 많다면 overfitting 등이 발생하므로, 효율적으로 쌓아야하고, 그런 모델을 찾는 것이 목표이다.

  - 1x1 convolution과 같이 channel의 depth를 줄여 parameter의 수를 줄이는 과정을 진행하게 되는데, 이때 channel의 depth를 줄이면 어떤 변화가 일어나나요?

    -> 기본적으로 연산량이 줄어든다. 검색해볼 것

- Kaggle을 활용한 인공지능 공부 (for P stage)

# 5. 피어세션 정리

- 강의 리뷰 및 QnA

# 6. 회고

- Transformer, Auto-Encoder, GAN는 열심히 복습해둘 것!
